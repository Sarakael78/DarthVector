#### Code Files ####

## updateVectorStoreTab.py ##

import tkinter as tk
from tkinter import filedialog, messagebox, ttk, scrolledtext
from pathlib import Path
import logging
import os
import threading
import tempfile
import shutil
from config import config
from dotenv import set_key, find_dotenv
from typing import Any
from embeddingGenerator import SentenceTransformerEmbedder
from vectorStore import VectorStore
from main import RtfProcessingPipeline
import gc

class UpdateVectorStoreTab(ttk.Frame):
    """Tab for updating the existing vector store with new files."""

    def __init__(self, parent: ttk.Notebook, app: Any):
        """Initialise the Update Vector Store Tab."""
        super().__init__(parent)
        self.parent = parent
        self.app = app
        self.logger = logging.getLogger(__name__)
        self.create_widgets()

    def create_widgets(self) -> None:
        """Create and layout widgets for the Update Vector Store tab."""
        # --- Existing Vector Store Paths ---
        existingFrame = ttk.LabelFrame(self, text="Existing Vector Store Paths")
        existingFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(existingFrame, text="Index Path:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.updateIndexEntry = ttk.Entry(existingFrame, width=60)
        self.updateIndexEntry.insert(0, str(config.defaultIndexPath))
        self.updateIndexEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")
        self.browseUpdateIndexButton = ttk.Button(existingFrame, text="Browse", command=self.browseUpdateIndexFile)
        self.browseUpdateIndexButton.grid(row=0, column=2, padx=5, pady=5)

        ttk.Label(existingFrame, text="Metadata Path:").grid(row=1, column=0, padx=5, pady=5, sticky="w")
        self.updateMetadataEntry = ttk.Entry(existingFrame, width=60)
        self.updateMetadataEntry.insert(0, str(config.defaultMetadataPath))
        self.updateMetadataEntry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")
        self.browseUpdateMetadataButton = ttk.Button(existingFrame, text="Browse", command=self.browseUpdateMetadataFile)
        self.browseUpdateMetadataButton.grid(row=1, column=2, padx=5, pady=5)

        # --- New Files Directory ---
        inputFrame = ttk.LabelFrame(self, text="New Files Directory")
        inputFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(inputFrame, text="Select Directory:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.inputDirEntry = ttk.Entry(inputFrame, width=60)
        self.inputDirEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")
        self.browseInputDirButton = ttk.Button(inputFrame, text="Browse", command=self.browseInputDir)
        self.browseInputDirButton.grid(row=0, column=2, padx=5, pady=5)

        # --- Processing Parameters ---
        paramsFrame = ttk.LabelFrame(self, text="Processing Parameters")
        paramsFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(paramsFrame, text="Model Name:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.modelNameEntry = ttk.Entry(paramsFrame, width=60)
        self.modelNameEntry.insert(0, config.defaultModelName)
        self.modelNameEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Max Workers:").grid(row=1, column=0, padx=5, pady=5, sticky="w")
        self.maxWorkersEntry = ttk.Entry(paramsFrame, width=10)
        self.maxWorkersEntry.insert(0, str(config.defaultMaxWorkers))
        self.maxWorkersEntry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Chunk Size:").grid(row=2, column=0, padx=5, pady=5, sticky="w")
        self.chunkSizeEntry = ttk.Entry(paramsFrame, width=10)
        self.chunkSizeEntry.insert(0, str(config.defaultChunkSize))
        self.chunkSizeEntry.grid(row=2, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Chunk Overlap:").grid(row=3, column=0, padx=5, pady=5, sticky="w")
        self.chunkOverlapEntry = ttk.Entry(paramsFrame, width=10)
        self.chunkOverlapEntry.insert(0, str(config.defaultChunkOverlap))
        self.chunkOverlapEntry.grid(row=3, column=1, padx=5, pady=5, sticky="ew")

        # --- Update Button ---
        self.updateButton = ttk.Button(self, text="Update Vector Store", command=self.startUpdateVectorStore)
        self.updateButton.pack(pady=20)

        # --- Progress Bar ---
        self.updateProgressBar = ttk.Progressbar(self, mode='determinate', maximum=100)
        self.updateProgressBar.pack(padx=10, pady=10, fill="x")

        # --- Status Text Area ---
        statusFrame = ttk.LabelFrame(self, text="Update Status")
        statusFrame.pack(padx=10, pady=10, fill="both", expand=True)
        self.updateStatusText = scrolledtext.ScrolledText(statusFrame, height=10, wrap=tk.WORD, state=tk.DISABLED)
        self.updateStatusText.pack(fill="both", expand=True)

        # --- Save Configuration Button ---
        self.saveConfigButton = ttk.Button(self, text="Save Configuration to .env", command=self.saveConfig)
        self.saveConfigButton.pack(pady=10)

    def browseUpdateIndexFile(self) -> None:
        """Open file selection dialogue for the existing FAISS index file."""
        filePath = filedialog.askopenfilename(defaultextension=".faiss", filetypes=[("FAISS Index Files", "*.faiss"), ("All Files", "*.*")])
        if filePath:
            self.updateIndexEntry.delete(0, tk.END)
            self.updateIndexEntry.insert(0, filePath)

    def browseUpdateMetadataFile(self) -> None:
        """Open file selection dialogue for the existing metadata file."""
        filePath = filedialog.askopenfilename(defaultextension=".pkl", filetypes=[("Pickle Files", "*.pkl"), ("All Files", "*.*")])
        if filePath:
            self.updateMetadataEntry.delete(0, tk.END)
            self.updateMetadataEntry.insert(0, filePath)

    def browseInputDir(self) -> None:
        """Open directory selection dialogue for new files."""
        directory = filedialog.askdirectory()
        if directory:
            self.inputDirEntry.delete(0, tk.END)
            self.inputDirEntry.insert(0, directory)

    def log_update_message(self, message: str) -> None:
        """Append message to status text area in the Update Vector Store tab."""
        self.updateStatusText.config(state=tk.NORMAL)
        self.updateStatusText.insert(tk.END, message + "\n")
        self.updateStatusText.see(tk.END)
        self.updateStatusText.config(state=tk.DISABLED)
        self.app.update_status(message)

    def validate_numeric_input(self, value, field_name, min_value=None, max_value=None):
        """Validate that a string can be converted to an integer within range."""
        try:
            num_value = int(value)
            if min_value is not None and num_value < min_value:
                raise ValueError(f"{field_name} must be at least {min_value}")
            if max_value is not None and num_value > max_value:
                raise ValueError(f"{field_name} must be less than or equal to {max_value}")
            return num_value
        except ValueError:
            if str(value).strip() == '':
                raise ValueError(f"{field_name} cannot be empty")
            raise ValueError(f"Invalid {field_name}: '{value}' is not a valid integer")          
   
    def validate_path(self, path_str, check_type="file", allow_create=False):
        """
        Validate a path string.
        
        Args:
            path_str: The path string to validate
            check_type: One of "file" or "dir" to check the path type
            allow_create: Whether to allow non-existent paths (for output files)
            
        Returns:
            The validated Path object
        
        Raises:
            ValueError: If the path is invalid
        """
        if not path_str or not path_str.strip():
            raise ValueError(f"Path cannot be empty")
            
        try:
            # Convert to absolute path and resolve to eliminate '..' components
            path = Path(path_str).resolve(strict=False)
            
            # Check if path exists
            if check_type == "file":
                if path.exists() and not path.is_file():
                    raise ValueError(f"Path exists but is not a file: {path}")
                if not allow_create and not path.exists():
                    raise ValueError(f"File does not exist: {path}")
            elif check_type == "dir":
                if path.exists() and not path.is_dir():
                    raise ValueError(f"Path exists but is not a directory: {path}")
                if not path.exists():
                    raise ValueError(f"Directory does not exist: {path}")
                    
            return path
        except Exception as e:
            if isinstance(e, ValueError):
                raise
            raise ValueError(f"Invalid path: {path_str} - {e}")

    def startUpdateVectorStore(self) -> None:
        """Initiate the process of updating the vector store."""
        # Disable the update button during validation to prevent multiple clicks
        self.updateButton.config(state=tk.DISABLED)
        
        try:
            indexPath = self.updateIndexEntry.get()
            metadataPath = self.updateMetadataEntry.get()
            
            # Validate paths
            try:
                self.validate_path(indexPath, "file", allow_create=True)
                self.validate_path(metadataPath, "file", allow_create=True)
                inputDir = self.validate_path(self.inputDirEntry.get(), "dir")
            except ValueError as e:
                messagebox.showerror("Error", f"Invalid path: {e}")
                self.updateButton.config(state=tk.NORMAL)  # Re-enable button on error
                return
            
            modelName = self.modelNameEntry.get()
            
            # Input validation
            try:
                maxWorkers = self.validate_numeric_input(self.maxWorkersEntry.get(), "Max Workers", min_value=1)
                chunkSize = self.validate_numeric_input(self.chunkSizeEntry.get(), "Chunk Size", min_value=1)
                chunkOverlap = self.validate_numeric_input(self.chunkOverlapEntry.get(), "Chunk Overlap", min_value=0)
                
                # Additional validation
                if chunkOverlap >= chunkSize:
                    raise ValueError("Chunk Overlap must be less than Chunk Size")

                os.makedirs(Path(indexPath).parent, exist_ok=True)
                os.makedirs(Path(metadataPath).parent, exist_ok=True)
            except ValueError as e:
                messagebox.showerror("Error", f"Invalid input: {e}")
                self.updateButton.config(state=tk.NORMAL)  # Re-enable button on error
                return

            if not messagebox.askyesno("Confirm Update", "Updating will overwrite the existing vector store. Are you sure you want to continue?"):
                self.updateButton.config(state=tk.NORMAL)  # Re-enable button on cancel
                return

            self.log_update_message("Starting vector store update...")
            self.log_update_message(f"Existing Index: {indexPath}")
            self.log_update_message(f"Existing Metadata: {metadataPath}")
            self.log_update_message(f"New Input Directory: {inputDir}")
            self.log_update_message(f"Model Name: {modelName}")
            self.log_update_message(f"Max Workers: {maxWorkers}")
            self.log_update_message(f"Chunk Size: {chunkSize}")
            self.log_update_message(f"Chunk Overlap: {chunkOverlap}")

            # Disable the update button during processing and start progress bar
            self.updateButtonOriginalStyle = self.updateButton.cget("style")
            disabled_style = "Disabled.TButton"
            style = ttk.Style()
            style.configure(disabled_style, background="lightgrey")
            self.updateButton.config(state=tk.DISABLED, style=disabled_style)
            self.updateProgressBar['value'] = 0
            
            # Process in background thread to prevent GUI freeze
            def process_in_thread():
                try:
                    # Track chunks processed instead of files
                    total_chunks_processed = [0]
                    estimated_total_chunks = [100]  # Start with placeholder value
                    
                    def update_progress(chunks_processed):
                        total_chunks_processed[0] += chunks_processed
                        # Update progress based on chunks
                        progress = min(100, int((total_chunks_processed[0] / max(estimated_total_chunks[0], 1)) * 100))
                        self.master.after(0, lambda: self._update_progress(progress))
                    
                    # Load existing vector store
                    embedGenerator = None
                    vectorStore = None
                    
                    try:
                        # Load existing vector store
                        embedGenerator = SentenceTransformerEmbedder(modelName)
                        vectorStore = VectorStore(embedGenerator.embeddingDim)
                        
                        # Check if files exist - if not, we'll create new ones
                        if os.path.exists(indexPath) and os.path.exists(metadataPath):
                            try:
                                vectorStore.load(indexPath, metadataPath)
                                self.master.after(0, lambda: self.log_update_message("Loaded existing vector store successfully"))
                            except Exception as e:
                                self.master.after(0, lambda: self.log_update_message(f"Failed to load existing vector store: {e}, creating new one"))
                        else:
                            self.master.after(0, lambda: self.log_update_message("Creating new vector store"))
                        
                        # Create processing pipeline
                        pipeline = RtfProcessingPipeline(
                            modelName=modelName,
                            chunkSize=chunkSize,
                            chunkOverlap=chunkOverlap,
                            maxWorkers=maxWorkers,
                            existingVectorStore=vectorStore
                        )
                        
                        # Estimate chunks per file to give better progress indication
                        # Get one sample file to estimate chunk count
                        for ext in config.supportedExtensions:
                            sample_files = list(inputDir.glob(f"*{ext}"))
                            if sample_files:
                                try:
                                    chunks = pipeline.processFile(sample_files[0])
                                    avg_chunks_per_file = max(1, chunks)  # At least 1
                                    # Count files to process
                                    total_files = sum(len(list(inputDir.glob(f"*{ext}"))) for ext in config.supportedExtensions)
                                    estimated_total_chunks[0] = total_files * avg_chunks_per_file
                                    self.master.after(0, lambda: self.log_update_message(
                                        f"Estimated {estimated_total_chunks[0]} chunks to process from {total_files} files"))
                                    break
                                except Exception:
                                    pass
                        
                        # Process directory with progress tracking
                        self.master.after(0, lambda: self.log_update_message("Processing files..."))
                        
                        # Define custom progress callback
                        def progress_callback(chunks_processed):
                            update_progress(chunks_processed)
                        
                        # Process directory with progress callback
                        processedFiles = pipeline.processDirectory(inputDir, progress_callback)
                        
                        # Save results
                        pipeline.saveResults(indexPath, metadataPath)
                        
                        # Update progress
                        self.master.after(0, lambda: self._update_progress_final(processedFiles, total_chunks_processed[0]))
                        
                    except Exception as e:
                        self.master.after(0, lambda: self._handle_error(e))
                    finally:
                        # Clean up resources even if an error occurred
                        try:
                            if embedGenerator:
                                del embedGenerator
                            if vectorStore:
                                vectorStore.release_model()
                                del vectorStore
                            SentenceTransformerEmbedder.release_models()
                            gc.collect()
                        except Exception as cleanup_error:
                            logging.error(f"Error during resource cleanup: {cleanup_error}")
                except Exception as e:
                    self.master.after(0, lambda: self._handle_error(e))
            
            # Start thread
            update_thread = threading.Thread(target=process_in_thread)
            update_thread.daemon = True
            update_thread.start()
        except Exception as e:
            # For any unexpected errors during setup
            self._handle_error(e)
    
    def _update_progress(self, progress):
        """Update progress bar value."""
        self.updateProgressBar['value'] = progress
        self.master.update_idletasks()
    
    def _update_progress_final(self, processed_files, total_chunks):
        """Update progress bar to final state."""
        self.updateProgressBar['value'] = 100
        
        self.log_update_message(f"Processed {processed_files} files with {total_chunks} chunks")
        messagebox.showinfo("Success", "Vector store updated successfully!")
        self.app.update_status("Vector store updated successfully.")
        self.updateButton.config(state=tk.NORMAL, style=self.updateButtonOriginalStyle)
    
    def _handle_error(self, e):
        """Handle errors in the UI thread."""
        self.app.logger.error(f"Update failed: {e}", exc_info=True)
        self.log_update_message(f"Error during update: {e}")
        messagebox.showerror("Error", f"An error occurred during update: {e}")
        self.app.update_status("Vector store update failed.")
        self.updateButton.config(state=tk.NORMAL, style=self.updateButtonOriginalStyle)

    def saveConfig(self) -> None:
        """Save the configuration from the GUI to the .env file."""
        index_path = self.updateIndexEntry.get()
        metadata_path = self.updateMetadataEntry.get()
        output_dir = Path(self.updateIndexEntry.get()).parent.as_posix()
        index_name = Path(self.updateIndexEntry.get()).name
        metadata_name = Path(self.updateMetadataEntry.get()).name
        model_name = self.modelNameEntry.get()
        max_workers = self.maxWorkersEntry.get()
        chunk_size = self.chunkSizeEntry.get()
        chunk_overlap = self.chunkOverlapEntry.get()

        dotenv_path = find_dotenv()
        if dotenv_path:
            set_key(dotenv_path, "DEFAULT_INDEX_PATH", index_path)
            set_key(dotenv_path, "DEFAULT_METADATA_PATH", metadata_path)
            set_key(dotenv_path, "DEFAULT_OUTPUT_DIR", output_dir)
            set_key(dotenv_path, "DEFAULT_INDEX_NAME", index_name)
            set_key(dotenv_path, "DEFAULT_METADATA_NAME", metadata_name)
            set_key(dotenv_path, "MODEL_NAME", model_name)
            set_key(dotenv_path, "MAX_WORKERS", max_workers)
            set_key(dotenv_path, "CHUNK_SIZE", chunk_size)
            set_key(dotenv_path, "CHUNK_OVERLAP", chunk_overlap)
            messagebox.showinfo("Success", "Update Vector Store configuration saved to .env. Please restart the application for the changes to fully take effect.")
            self.app.update_status("Update Vector Store configuration saved to .env")
        else:
            messagebox.showerror("Error", ".env file not found.")
            self.app.update_status("Error: .env file not found.")

## embeddingGenerator.py ##

import logging
import threading
import time
from abc import ABC, abstractmethod
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Optional, Any, Dict
from config import config
import gc

class BaseEmbedder(ABC):
    """Abstract base class for embedding generators."""

    @abstractmethod
    def encodeChunks(self, chunks: List[str]) -> np.ndarray:
        """Generate embeddings for text chunks."""
        pass

    @property
    @abstractmethod
    def embeddingDim(self) -> int:
        """Return the embedding dimension."""
        pass

class SentenceTransformerEmbedder(BaseEmbedder):
    """Generates embeddings using SentenceTransformer models."""

    _model_instances: Dict[str, SentenceTransformer] = {}
    _lock = threading.RLock()  # Using RLock for thread-safety
    
    def __init__(self, modelName: Optional[str] = None, batchSize: int = 32, 
                 maxWorkers: Optional[int] = None, existing_model: Optional[Any] = None) -> None:
        """
        Initialise the embedder.

        Args:
            modelName: Name of the SentenceTransformer model (defaults to config.modelName).
            batchSize: Batch size for encoding.
            maxWorkers: Number of threads for parallel encoding (currently unused).
            existing_model: Optional existing model instance to reuse.

        Raises:
            RuntimeError: If model loading fails.
        """
        self.modelName = modelName if modelName is not None else config.defaultModelName
        self.batchSize = batchSize
        self.maxWorkers = maxWorkers  # Unused but retained for compatibility
        self.process_id = id(threading.current_thread())  # Use thread ID instead of process ID
        self.logger = logging.getLogger(__name__)
        
        try:
            # Use existing model if provided
            if existing_model is not None:
                self.model = existing_model
                self._embeddingDim = self.model.get_sentence_embedding_dimension()
                self.logger.info(f"Using existing model, dimension: {self._embeddingDim}")
            else:
                # Get or create model for this thread
                self._get_model_for_thread()
        except Exception as e:
            self.logger.error(f"Failed to load model '{self.modelName}': {e}", exc_info=True)
            raise RuntimeError(f"Failed to load model '{self.modelName}': {e}")

    def _get_model_for_thread(self):
        """Get or create a model instance for the current thread with proper locking."""
        thread_id = self.process_id
        model_key = f"{thread_id}_{self.modelName}"
        
        with self._lock:
            if model_key in SentenceTransformerEmbedder._model_instances:
                self.model = SentenceTransformerEmbedder._model_instances[model_key]
                self._embeddingDim = self.model.get_sentence_embedding_dimension()
                self.logger.debug(f"Thread {thread_id}: Reusing existing model instance for {self.modelName}")
                return
            
            self.logger.info(f"Thread {thread_id}: Loading model '{self.modelName}'...")
            startTime = time.time()
            
            # Load the model for this thread
            self.model = SentenceTransformer(self.modelName)
            self._embeddingDim = self.model.get_sentence_embedding_dimension()
            
            # Store in dictionary with thread-specific key
            SentenceTransformerEmbedder._model_instances[model_key] = self.model
            
            self.logger.info(f"Thread {thread_id}: Model '{self.modelName}' loaded in {time.time() - startTime:.2f}s, dimension: {self._embeddingDim}")

    def encodeChunks(self, chunks: List[str]) -> np.ndarray:
        """
        Generate embeddings for text chunks.

        Args:
            chunks: List of text chunks.

        Returns:
            np.ndarray: Array of embeddings (shape: len(chunks) x embedding_dim).

        Raises:
            RuntimeError: If encoding fails.
        """
        if not chunks:
            return np.array([], dtype=np.float32).reshape(0, self.embeddingDim)
        try:
            return self.model.encode(chunks, batch_size=self.batchSize, convert_to_numpy=True, show_progress_bar=False)
        except Exception as e:
            self.logger.error(f"Encoding failed: {e}", exc_info=True)
            raise RuntimeError(f"Failed to encode chunks: {e}")

    @property
    def embeddingDim(self) -> int:
        """Return the embedding dimension."""
        return self._embeddingDim
    
    @classmethod
    def release_models(cls) -> None:
        """Release all model instances from memory with proper error handling."""
        try:
            with cls._lock:
                # Make a copy of keys to avoid dictionary modification during iteration
                model_keys = list(cls._model_instances.keys())
                for key in model_keys:
                    try:
                        if key in cls._model_instances:
                            del cls._model_instances[key]
                    except Exception as e:
                        logging.error(f"Error releasing model {key}: {e}")
                cls._model_instances.clear()
                # Force garbage collection to ensure models are freed
                gc.collect()
            logging.info("All SentenceTransformer model instances released from memory.")
        except Exception as e:
            logging.error(f"Error in release_models: {e}", exc_info=True)
    
    @staticmethod
    def _safe_cleanup():
        """Safely clean up resources even if an exception occurs."""
        try:
            SentenceTransformerEmbedder.release_models()
            gc.collect()
            logging.info("SentenceTransformer models released from memory")
        except Exception as e:
            logging.error(f"Error during SentenceTransformer cleanup: {e}", exc_info=True)

## textExtractor.py ##

# textExtractor.py
import logging
from pathlib import Path
import pdfplumber
from docx import Document
from striprtf.striprtf import rtf_to_text
from config import config
from exceptions import FileProcessingError, TextExtractionError, FileSizeExceededError, UnsupportedFileTypeError, RTFEncodingError


def extractTextFromFile(filePath: Path) -> str:
    """
    Extract text from a file based on its extension.

    Args:
        filePath: Path to the file.

    Returns:
        str: Extracted text.

    Raises:
        FileNotFoundError: If the file doesn't exist.
        FileSizeExceededError: If the file exceeds the size limit.
        UnsupportedFileTypeError: If the file type is not supported.
        TextExtractionError: If text extraction fails.
    """
    path = filePath.resolve()
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")

    fileSizeMb = path.stat().st_size / (1024 * 1024)
    if fileSizeMb > config.maxFileSizeMB:
        raise FileSizeExceededError(f"File size ({fileSizeMb:.2f}MB) exceeds limit ({config.maxFileSizeMB}MB)")

    ext = path.suffix.lower()
    if ext == '.rtf':
        return extractTextFromRtf(path)
    elif ext == '.pdf':
        return extractTextFromPdf(path)
    elif ext == '.docx':
        return extractTextFromDocx(path)
    else:
        raise UnsupportedFileTypeError(f"Unsupported file type: {ext}")

def extractTextFromRtf(filePath: Path) -> str:
    """
    Extract text from an RTF file.
    
    Args:
        filePath: Path to the RTF file
        
    Returns:
        str: Extracted text
        
    Raises:
        TextExtractionError: If text extraction fails
    """
    try:
        # Try multiple encodings
        encodings = ["utf-8", "latin-1", "cp1252", "iso-8859-1", "windows-1252"]
        rtfContent = None
        decode_errors = []
        
        for encoding in encodings:
            try:
                rtfContent = filePath.read_text(encoding=encoding)
                logging.debug(f"Successfully decoded {filePath} using {encoding} encoding")
                break
            except UnicodeDecodeError as e:
                decode_errors.append(f"{encoding}: {str(e)}")
                continue
                
        if rtfContent is None:
            error_msg = f"Failed to decode RTF file {filePath} with encodings: {', '.join(encodings)}"
            logging.error(error_msg)
            logging.debug(f"Decode errors: {decode_errors}")
            raise TextExtractionError(error_msg)
        
        text = rtf_to_text(rtfContent)
        logging.info(f"Extracted {len(text)} characters from {filePath}")
        return text
    except Exception as e:
        logging.error(f"Failed to extract text from RTF {filePath}: {e}")
        raise TextExtractionError(f"Error extracting text from RTF {filePath}: {e}")

def extractTextFromPdf(filePath: Path) -> str:
    """
    Extract text from a PDF file using pdfplumber.
    
    Args:
        filePath: Path to the PDF file
        
    Returns:
        str: Extracted text
        
    Raises:
        TextExtractionError: If text extraction fails
    """
    try:
        with pdfplumber.open(filePath) as pdf:
            text_parts = []
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:  # Check if page_text is not None and not empty
                    text_parts.append(page_text)
                else:
                    logging.warning(f"No text extracted from page {page.page_number} in {filePath}. Possibly an image-only page.")
            
            if not text_parts:
                logging.warning(f"{filePath} appears to be an image-based PDF with no extractable text.")
                return f"[Image-based PDF: No text could be extracted from {filePath.name}]"
                
            text = "\n".join(text_parts)
            logging.info(f"Extracted {len(text)} characters from {filePath}")
            return text
    except Exception as e:
        logging.error(f"Failed to extract text from PDF {filePath}: {e}")
        raise TextExtractionError(f"Error extracting text from PDF {filePath}: {e}")

def extractTextFromDocx(filePath: Path) -> str:
    """Extract text from a DOCX file using python-docx."""
    try:
        doc = Document(filePath)
        text = "\n".join(para.text for para in doc.paragraphs)
        logging.info(f"Extracted {len(text)} characters from {filePath}")
        return text
    except Exception as e:
        logging.error(f"Failed to extract text from DOCX {filePath}: {e}")
        raise TextExtractionError(f"Error extracting text from DOCX {filePath}: {e}")

## testPipeline.py ##

import pytest
import tempfile
from pathlib import Path
import numpy as np
from rtfProcessor import extractTextFromRtf, RTFEncodingError, FileSizeExceededError
from textPreprocessor import TextPreprocessor
from vectorStore import VectorStore
from main import RtfProcessingPipeline
from config import config


@pytest.fixture
def tempRtfFile():
    with tempfile.NamedTemporaryFile(suffix=".rtf", delete=False) as f:
        f.write(b"{\\rtf1\\ansi This is a test}")
    file_path = Path(f.name)
    yield file_path
    file_path.unlink(missing_ok=True)


@pytest.fixture
def tempLargeRtfFile():
    with tempfile.NamedTemporaryFile(suffix=".rtf", delete=False) as f:
        f.write(b"{\\rtf1\\ansi " + b"A" * int(0.1 * 1024 * 1024) + b"}")
    file_path = Path(f.name)
    yield file_path
    file_path.unlink(missing_ok=True)


@pytest.fixture
def tempNonRtfFile():
    with tempfile.NamedTemporaryFile(suffix=".txt", delete=False) as f:
        f.write(b"This is not an RTF file.")
    file_path = Path(f.name)
    yield file_path
    file_path.unlink(missing_ok=True)


@pytest.fixture
def tempRtfFileWithEncoding():
    content = "{\\rtf1\\ansi\\deff0{\\fonttbl{\\f0 Arial;}}This is a test with some special characters: éàçüö}"
    with tempfile.NamedTemporaryFile(suffix=".rtf", delete=False, mode="w", encoding="latin-1") as f:
        f.write(content)
    file_path = Path(f.name)
    yield file_path
    file_path.unlink(missing_ok=True)


class TestTextPreprocessor:
    def testCleanText(self):
        text = "This is\n\na test   with\tspaces"
        assert TextPreprocessor.cleanText(text) == "This is a test with spaces"

    def testSegmentText(self):
        preprocessor = TextPreprocessor(maxWords=5, chunkOverlap=2)
        text = "This is a test of the segmentation function with overlapping words"
        segments = preprocessor.segmentText(text)
        assert len(segments) == 3
        assert segments[0] == "This is a test of"
        assert segments[1] == "of the segmentation function with"
        assert "with overlapping words" in segments[2]

    def testSegmentTextEmpty(self):
        preprocessor = TextPreprocessor(maxWords=5, chunkOverlap=2)
        text = ""
        segments = preprocessor.segmentText(text)
        assert segments == []


class TestVectorStore:
    def testAddSearch(self):
        embeddingDimension = 3
        store = VectorStore(embeddingDimension=embeddingDimension)
        embeddings = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)
        metadata = [{"id": i} for i in range(3)]
        store.addEmbeddings(embeddings, metadata)
        assert store.index.ntotal == 3
        queryEmbedding = np.array([1, 1, 1], dtype=np.float32)
        distances, _, results = store.search(queryEmbedding, topK=2)
        assert len(results) == 2
        assert results[0]["id"] == 0

    def testSaveLoad(self):
        embeddingDimension = 3
        store = VectorStore(embeddingDimension=embeddingDimension)
        embeddings = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)
        metadata = [{"id": 1}, {"id": 2}]
        store.addEmbeddings(embeddings, metadata)

        with tempfile.TemporaryDirectory() as tempDir:
            indexPath = Path(tempDir) / "test_index.faiss"
            metadataPath = Path(tempDir) / "test_metadata.pkl"
            store.save(indexPath=str(indexPath), metadataPath=str(metadataPath))

            loadedStore = VectorStore(embeddingDimension=embeddingDimension)
            loadedStore.load(indexPath=str(indexPath), metadataPath=str(metadataPath))

            assert loadedStore.index.ntotal == 2
            assert len(loadedStore.metadata) == 2
            assert loadedStore.metadata[0]["id"] == 1

    def testSearchEmpty(self):
        embeddingDimension = 3
        store = VectorStore(embeddingDimension=embeddingDimension)
        queryEmbedding = np.array([1, 2, 3], dtype=np.float32)
        distances, indices, metadata = store.search(queryEmbedding, topK=1)
        assert len(distances) == 0
        assert len(indices) == 0
        assert len(metadata) == 0


class TestRtfProcessor:
    def testExtractTextFromRtf(self, tempRtfFile):
        text = extractTextFromRtf(tempRtfFile)
        assert "This is a test" in text

    def testExtractTextFromRtfFileNotFound(self):
        with pytest.raises(FileNotFoundError):
            extractTextFromRtf("non_existent_file.rtf")

    def testExtractTextFromRtfFileSizeExceeded(self, tempLargeRtfFile):
        config.maxFileSizeMB = 0.00001  # Very small limit
        try:
            with pytest.raises(FileSizeExceededError):
                extractTextFromRtf(tempLargeRtfFile)
        finally:
            config.maxFileSizeMB = 100  # Reset config

    def testExtractTextFromNonRtfFile(self, tempNonRtfFile):
        with pytest.raises(RTFEncodingError):
            extractTextFromRtf(tempNonRtfFile)

    def testExtractTextWithSpecialCharacters(self, tempRtfFileWithEncoding):
        text = extractTextFromRtf(tempRtfFileWithEncoding)
        assert "This is a test with some special characters: éàçüö" in text


class TestPipeline:
    def testProcessFile(self, tempRtfFile):
        pipeline = RtfProcessingPipeline(chunkSize=5, maxWorkers=1)
        chunks = pipeline.processFile(tempRtfFile)
        assert chunks > 0
        # Ensure that the number of vectors in the store matches the number of processed chunks
        assert pipeline.vectorStore.index.ntotal == chunks

    def testProcessFileErrorHandling(self, tempNonRtfFile):
        pipeline = RtfProcessingPipeline(chunkSize=5, maxWorkers=1)
        chunks = pipeline.processFile(tempNonRtfFile)
        assert chunks == 0
        assert pipeline.vectorStore.index.ntotal == 0

    def testProcessDirectory(self, tempRtfFile, tmp_path):
        # Create a temporary directory with multiple RTF files using tmp_path
        tempDir = tmp_path / "rtf_files"
        tempDir.mkdir()
        numFiles = 3
        for i in range(numFiles):
            file_path = tempDir / f"test_{i}.rtf"
            file_path.write_text("{\\rtf1\\ansi Test file content}")
        pipeline = RtfProcessingPipeline(chunkSize=10, maxWorkers=2)
        processedFiles = pipeline.processDirectory(tempDir)
        assert processedFiles == numFiles
        assert pipeline.vectorStore.index.ntotal == numFiles  # Each file should produce one chunk

    def testProcessDirectoryNoRtfFiles(self, tmp_path):
        tempDir = tmp_path / "empty_dir"
        tempDir.mkdir()
        pipeline = RtfProcessingPipeline(chunkSize=10, maxWorkers=2)
        processedFiles = pipeline.processDirectory(tempDir)
        assert processedFiles == 0
        assert pipeline.vectorStore.index.ntotal == 0

## vectorStore.py ##

import faiss
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import threading  # Changed from multiprocessing to threading
from config import config
import os
import shutil

class VectorStore:
    """Class that manages a FAISS vector store with associated metadata."""
    
    def __init__(self, embeddingDimension: int, modelName: str = config.defaultModelName):
        """
        Initialize a new vector store.
        
        Args:
            embeddingDimension: Dimension of the embeddings to store
            modelName: Name of the sentence transformer model
        """
        self.embeddingDimension = embeddingDimension
        self.index = faiss.IndexFlatL2(embeddingDimension)
        self.metadata: List[Dict] = []
        self.embedder: Optional[SentenceTransformer] = None  # Load on demand
        
        # Use a thread-safe lock for all operations
        self._lock = threading.RLock()  # RLock allows re-entry by same thread
        
        logging.info(f"FAISS index created with dimension {embeddingDimension}")

    def addEmbeddings(self, embeddings, metadatas):
        """
        Add embeddings and their metadata to the vector store.
        
        Args:
            embeddings: Numpy array of embedding vectors
            metadatas: List of metadata dictionaries corresponding to each embedding
        """
        with self._lock:
            self.index.add(embeddings)
            self.metadata.extend(metadatas)
            logging.info(f"Added {len(embeddings)} embeddings")

    def search(self, query_embedding: np.ndarray, topK: int = 5) -> tuple[np.ndarray, np.ndarray, List[Dict]]:
        """
        Search the vector store for similar embeddings.

        Args:
            query_embedding: The query embedding vector.
            topK: Number of results to return.

        Returns:
            Tuple of (distances, indices, metadata) for the closest vectors.
        """
        if self.index.ntotal == 0:
            logging.error("Vector store is empty.")
            return np.array([]), np.array([]), []
            
        with self._lock:
            distances, indices = self.index.search(query_embedding.reshape(1, -1), topK)
            results = []
            for i in range(len(indices[0])):
                idx = indices[0][i]
                # Added bounds checking for metadata indices
                if idx != -1 and idx < len(self.metadata):  
                    metadata = self.metadata[idx].copy()  # Make a copy to avoid modifying the original
                    metadata['distance'] = float(distances[0][i])  # Convert to native Python type
                    results.append(metadata)
        return distances, indices, results
    
    def search_by_text(self, query: str, topK: int = 5) -> List[Dict]:
        """
        Search the vector store using a text query.

        Args:
            query: The text query.
            topK: Number of results to return.

        Returns:
            List of metadata dictionaries for the closest vectors.
        """
        if self.embedder is None:
            logging.error("Embedder model is not loaded.")
            return []
        vector = self.embedder.encode([query])[0]
        _, _, results = self.search(vector, topK)
        return results

    def save(self, indexPath: Optional[str] = None, metadataPath: Optional[str] = None) -> None:
        """
        Save the index and metadata to disk with atomic operations for safety.

        Args:
            indexPath: Path for FAISS index (defaults to config.defaultIndexPath).
            metadataPath: Path for metadata (defaults to config.defaultMetadataPath).

        Raises:
            RuntimeError: If saving fails.
        """
        resolvedIndexPath = Path(indexPath or config.defaultIndexPath).resolve()
        resolvedMetadataPath = Path(metadataPath or config.defaultMetadataPath).resolve()
        config.ensureOutputDir()
        
        # Create temporary file paths for atomic writes
        temp_index_path = str(resolvedIndexPath) + ".temp"
        temp_metadata_path = str(resolvedMetadataPath) + ".temp"

        try:
            with self._lock:
                # Write to temporary files first
                faiss.write_index(self.index, temp_index_path)
                with open(temp_metadata_path, "wb") as f:
                    pickle.dump(self.metadata, f)
                
                # Rename temp files to final names (atomic operation)
                shutil.move(temp_index_path, str(resolvedIndexPath))
                shutil.move(temp_metadata_path, str(resolvedMetadataPath))
                
            logging.info(f"VectorStore saved to {resolvedIndexPath} and {resolvedMetadataPath}")
        except Exception as e:
            # Clean up temp files if they exist
            if os.path.exists(temp_index_path):
                os.remove(temp_index_path)
            if os.path.exists(temp_metadata_path):
                os.remove(temp_metadata_path)
            logging.error(f"VectorStore save failed: {e}")
            raise RuntimeError(f"Failed to save vector store: {e}")

    def load(self, indexPath: str, metadataPath: str, modelName: str = config.defaultModelName) -> None:
        """
        Load the index and metadata from disk with proper error handling.
        
        Args:
            indexPath: Path to the FAISS index file
            metadataPath: Path to the metadata file
            modelName: Name of the sentence transformer model
            
        Raises:
            FileNotFoundError: If index or metadata file is not found
            Exception: For other loading errors
        """
        resolvedIndexPath = Path(indexPath).resolve()
        resolvedMetadataPath = Path(metadataPath).resolve()
        
        # Check if files exist before attempting to load
        if not resolvedIndexPath.exists():
            raise FileNotFoundError(f"Index file not found: {resolvedIndexPath}")
        if not resolvedMetadataPath.exists():
            raise FileNotFoundError(f"Metadata file not found: {resolvedMetadataPath}")
            
        try:
            with self._lock:
                self.index = faiss.read_index(str(resolvedIndexPath))
                with resolvedMetadataPath.open("rb") as f:
                    self.metadata = pickle.load(f)
                self.embeddingDimension = self.index.d
                self.embedder = SentenceTransformer(modelName)
            logging.info(f"VectorStore loaded from {resolvedIndexPath} and {resolvedMetadataPath}")
        except FileNotFoundError:
            logging.error("Index or metadata file not found.")
            raise
        except Exception as e:
            logging.error(f"Error loading VectorStore: {e}")
            raise

    def release_model(self) -> None:
        """Releases the SentenceTransformer model from memory."""
        with self._lock:
            if self.embedder:
                del self.embedder
                self.embedder = None
                logging.info("SentenceTransformer model released from memory.")

## fileProcessingTab.py ##

# fileProcessingTab.py
import tkinter as tk
from tkinter import filedialog, messagebox, ttk, scrolledtext
from pathlib import Path
import logging
import os
import threading
import json
from typing import Any
from dotenv import set_key, find_dotenv
from config import config
from main import RtfProcessingPipeline
import gc

class FileProcessingTab(ttk.Frame):
    """Tab for processing files and updating vector store."""

    def __init__(self, parent: ttk.Notebook, app: Any):
        """Initialise the File Processing Tab."""
        super().__init__(parent)
        self.parent = parent
        self.app = app
        self.logger = logging.getLogger(__name__)
        self.files_to_process = 0  # Local counter for this tab
        self.create_widgets()

    def create_widgets(self) -> None:
        """Create and layout widgets for the File Processing tab."""
        # --- Vector Store Paths Section ---
        storeFrame = ttk.LabelFrame(self, text="Vector Store Paths")
        storeFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(storeFrame, text="Index Path:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.indexEntry = ttk.Entry(storeFrame, width=60)
        self.indexEntry.insert(0, str(config.defaultIndexPath))
        self.indexEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")
        self.browseIndexButton = ttk.Button(storeFrame, text="Browse", command=self.browseIndexFile)
        self.browseIndexButton.grid(row=0, column=2, padx=5, pady=5)

        ttk.Label(storeFrame, text="Metadata Path:").grid(row=1, column=0, padx=5, pady=5, sticky="w")
        self.metadataEntry = ttk.Entry(storeFrame, width=60)
        self.metadataEntry.insert(0, str(config.defaultMetadataPath))
        self.metadataEntry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")
        self.browseMetadataButton = ttk.Button(storeFrame, text="Browse", command=self.browseMetadataFile)
        self.browseMetadataButton.grid(row=1, column=2, padx=5, pady=5)

        # --- Input Files Section ---
        inputFrame = ttk.LabelFrame(self, text="Input Files")
        inputFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(inputFrame, text="Select Directory:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.inputDirEntry = ttk.Entry(inputFrame, width=60)
        self.inputDirEntry.insert(0, str(config.defaultInputDir))
        self.inputDirEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")
        self.browseInputDirButton = ttk.Button(inputFrame, text="Browse", command=self.browseInputDir)
        self.browseInputDirButton.grid(row=0, column=2, padx=5, pady=5)

        # --- Processing Parameters Section ---
        paramsFrame = ttk.LabelFrame(self, text="Processing Parameters")
        paramsFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(paramsFrame, text="Model Name:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.modelNameEntry = ttk.Entry(paramsFrame, width=60)
        self.modelNameEntry.insert(0, config.defaultModelName)
        self.modelNameEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Max Workers:").grid(row=1, column=0, padx=5, pady=5, sticky="w")
        self.maxWorkersEntry = ttk.Entry(paramsFrame, width=10)
        self.maxWorkersEntry.insert(0, str(config.defaultMaxWorkers))
        self.maxWorkersEntry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Chunk Size:").grid(row=2, column=0, padx=5, pady=5, sticky="w")
        self.chunkSizeEntry = ttk.Entry(paramsFrame, width=10)
        self.chunkSizeEntry.insert(0, str(config.defaultChunkSize))
        self.chunkSizeEntry.grid(row=2, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Chunk Overlap:").grid(row=3, column=0, padx=5, pady=5, sticky="w")
        self.chunkOverlapEntry = ttk.Entry(paramsFrame, width=10)
        self.chunkOverlapEntry.insert(0, str(config.defaultChunkOverlap))
        self.chunkOverlapEntry.grid(row=3, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(paramsFrame, text="Max File Size (MB):").grid(row=4, column=0, padx=5, pady=5, sticky="w")
        self.maxFileSizeEntry = ttk.Entry(paramsFrame, width=10)
        self.maxFileSizeEntry.insert(0, str(config.maxFileSizeMB))
        self.maxFileSizeEntry.grid(row=4, column=1, padx=5, pady=5, sticky="ew")

        # --- Multiprocessing Configuration ---
        multiprocessingFrame = ttk.LabelFrame(self, text="Multiprocessing")
        multiprocessingFrame.pack(padx=10, pady=10, fill="x")

        self.disableMultiprocessingVar = tk.BooleanVar(value=config.disableMultiprocessing)
        self.disableMultiprocessingCheckbutton = ttk.Checkbutton(
            multiprocessingFrame,
            text="Disable Multiprocessing (Use single worker)",
            variable=self.disableMultiprocessingVar
        )
        self.disableMultiprocessingCheckbutton.pack(padx=5, pady=5, fill="x")

        # --- Process Button ---
        buttonFrame = ttk.Frame(self)
        buttonFrame.pack(padx=10, pady=10, fill="x")
        
        self.processButton = ttk.Button(buttonFrame, text="Process Files / Update Vector Store", command=self.startProcessing)
        self.processButton.pack(pady=5, fill="x")

        # --- Progress Bar ---
        self.progressBar = ttk.Progressbar(self, mode='determinate', maximum=100)
        self.progressBar.pack(padx=10, pady=10, fill="x")

        # --- Status Text Area ---
        statusFrame = ttk.LabelFrame(self, text="Status")
        statusFrame.pack(padx=10, pady=10, fill="both", expand=True)
        self.statusText = scrolledtext.ScrolledText(statusFrame, height=10, wrap=tk.WORD, state=tk.DISABLED)
        self.statusText.pack(fill="both", expand=True)

        # --- Save Configuration Button ---
        self.saveConfigButton = ttk.Button(self, text="Save Configuration to .env", command=self.saveConfig)
        self.saveConfigButton.pack(pady=10)

    def browseIndexFile(self) -> None:
        """Open file selection dialogue for the FAISS index file."""
        filePath = filedialog.askopenfilename(defaultextension=".faiss", filetypes=[("FAISS Index Files", "*.faiss"), ("All Files", "*.*")])
        if filePath:
            self.indexEntry.delete(0, tk.END)
            self.indexEntry.insert(0, filePath)

    def browseMetadataFile(self) -> None:
        """Open file selection dialogue for the metadata file."""
        filePath = filedialog.askopenfilename(defaultextension=".pkl", filetypes=[("Pickle Files", "*.pkl"), ("All Files", "*.*")])
        if filePath:
            self.metadataEntry.delete(0, tk.END)
            self.metadataEntry.insert(0, filePath)

    def browseInputDir(self) -> None:
        """Open directory selection dialogue for input files."""
        directory = filedialog.askdirectory()
        if directory:
            self.inputDirEntry.delete(0, tk.END)
            self.inputDirEntry.insert(0, directory)

    def log_message(self, message: str) -> None:
        """Append message to status text area."""
        self.statusText.config(state=tk.NORMAL)
        self.statusText.insert(tk.END, message + "\n")
        self.statusText.see(tk.END)
        self.statusText.config(state=tk.DISABLED)
        self.app.update_status(message)

    def validate_numeric_input(self, value, field_name, min_value=None, max_value=None):
        """Validate that a string can be converted to an integer within range."""
        try:
            num_value = int(value)
            if min_value is not None and num_value < min_value:
                raise ValueError(f"{field_name} must be at least {min_value}")
            if max_value is not None and num_value > max_value:
                raise ValueError(f"{field_name} must be less than or equal to {max_value}")
            return num_value
        except ValueError:
            if str(value).strip() == '':
                raise ValueError(f"{field_name} cannot be empty")
            raise ValueError(f"Invalid {field_name}: '{value}' is not a valid integer")
    
    def validate_path(self, path_str, check_type="file", allow_create=False):
        """
        Validate a path string.
        
        Args:
            path_str: The path string to validate
            check_type: One of "file" or "dir" to check the path type
            allow_create: Whether to allow non-existent paths (for output files)
            
        Returns:
            The validated Path object
        
        Raises:
            ValueError: If the path is invalid
        """
        if not path_str or not path_str.strip():
            raise ValueError(f"Path cannot be empty")
            
        try:
            # Convert to absolute path and resolve to eliminate '..' components
            path = Path(path_str).resolve(strict=False)
            
            # Check if path exists
            if check_type == "file":
                if path.exists() and not path.is_file():
                    raise ValueError(f"Path exists but is not a file: {path}")
                if not allow_create and not path.exists():
                    raise ValueError(f"File does not exist: {path}")
            elif check_type == "dir":
                if path.exists() and not path.is_dir():
                    raise ValueError(f"Path exists but is not a directory: {path}")
                if not path.exists():
                    raise ValueError(f"Directory does not exist: {path}")
                    
            return path
        except Exception as e:
            if isinstance(e, ValueError):
                raise
            raise ValueError(f"Invalid path: {path_str} - {e}")

    def startProcessing(self) -> None:
        """Initiate the file processing pipeline."""
        # Disable the process button during validation to prevent multiple clicks
        self.processButton.config(state=tk.DISABLED)
        
        try:
            # Validate inputs
            indexPath = self.indexEntry.get()
            metadataPath = self.metadataEntry.get()
            
            # Validate paths
            try:
                self.validate_path(indexPath, "file", allow_create=True)
                self.validate_path(metadataPath, "file", allow_create=True)
                inputDir = self.validate_path(self.inputDirEntry.get(), "dir")
            except ValueError as e:
                messagebox.showerror("Error", f"Invalid path: {e}")
                self.processButton.config(state=tk.NORMAL)  # Re-enable button on error
                return
            
            modelName = self.modelNameEntry.get()
            
            # Input validation for numeric fields
            try:
                maxWorkers = self.validate_numeric_input(self.maxWorkersEntry.get(), "Max Workers", min_value=1)
                chunkSize = self.validate_numeric_input(self.chunkSizeEntry.get(), "Chunk Size", min_value=1)
                chunkOverlap = self.validate_numeric_input(self.chunkOverlapEntry.get(), "Chunk Overlap", min_value=0)
                maxFileSizeMB = self.validate_numeric_input(self.maxFileSizeEntry.get(), "Max File Size (MB)", min_value=1)
                
                # Additional validation
                if chunkOverlap >= chunkSize:
                    raise ValueError("Chunk Overlap must be less than Chunk Size")
            except ValueError as e:
                messagebox.showerror("Error", f"Invalid input: {e}")
                self.processButton.config(state=tk.NORMAL)  # Re-enable button on error
                return
            
            # Ensure output directories exist
            os.makedirs(Path(indexPath).parent, exist_ok=True)
            os.makedirs(Path(metadataPath).parent, exist_ok=True)
            
            # Check if we're updating an existing vector store
            updating = os.path.exists(indexPath) and os.path.exists(metadataPath)
            operation_type = "updating" if updating else "creating"
            
            if updating:
                message = "You are about to update the existing vector store with new files. " \
                          "New files will be appended to the store. Continue?"
            else:
                message = "You are about to create a new vector store. Continue?"
            
            if not messagebox.askyesno("Confirm Operation", message):
                self.processButton.config(state=tk.NORMAL)  # Re-enable button on cancel
                return
            
            disableMultiprocessing = self.disableMultiprocessingVar.get()

            # Log the operation we're about to perform
            self.log_message(f"Starting {operation_type} vector store...")
            self.log_message(f"Index Path: {indexPath}")
            self.log_message(f"Metadata Path: {metadataPath}")
            self.log_message(f"Input Directory: {inputDir}")
            self.log_message(f"Model Name: {modelName}")
            self.log_message(f"Max Workers: {maxWorkers}")
            self.log_message(f"Chunk Size: {chunkSize}")
            self.log_message(f"Chunk Overlap: {chunkOverlap}")
            self.log_message(f"Max File Size (MB): {maxFileSizeMB}")
            self.log_message(f"Disable Multiprocessing: {disableMultiprocessing}")

            # Disable the process button during processing
            self.processButtonOriginalStyle = self.processButton.cget("style")
            disabled_style = "Disabled.TButton"
            style = ttk.Style()
            style.configure(disabled_style, background="lightgrey")
            self.processButton.config(state=tk.DISABLED, style=disabled_style)
            self.progressBar['value'] = 0
            
            # Process in background thread to prevent GUI freeze
            def process_in_thread():
                try:
                    # Track progress
                    total_chunks_processed = [0]
                    estimated_total_chunks = [100]  # Start with placeholder value
                    
                    def update_progress(chunks_processed):
                        total_chunks_processed[0] += chunks_processed
                        # Update progress based on chunks
                        progress = min(100, int((total_chunks_processed[0] / max(estimated_total_chunks[0], 1)) * 100))
                        self.master.after(0, lambda: self._update_progress(progress))
                    
                    # Create processing pipeline
                    pipeline = RtfProcessingPipeline(
                        modelName=modelName,
                        chunkSize=chunkSize,
                        chunkOverlap=chunkOverlap,
                        maxWorkers=maxWorkers,
                        maxFileSizeMB=maxFileSizeMB,
                        useMultiprocessing=not disableMultiprocessing
                    )
                    
                    # Process directory with recursive file search
                    self.master.after(0, lambda: self.log_message("Processing files recursively..."))
                    
                    # Define custom progress callback
                    def progress_callback(chunks_processed):
                        update_progress(chunks_processed)
                    
                    # Process directory with progress callback
                    processedFiles = pipeline.processDirectory(inputDir, progress_callback)
                    
                    # Save results
                    pipeline.saveResults(indexPath, metadataPath)
                    
                    # Update progress
                    self.master.after(0, lambda: self._update_progress_final(processedFiles, total_chunks_processed[0]))
                    
                except Exception as e:
                    self.master.after(0, lambda: self._handle_error(e))
            
            # Start thread
            processing_thread = threading.Thread(target=process_in_thread)
            processing_thread.daemon = True
            processing_thread.start()
        except Exception as e:
            # For any unexpected errors during setup
            self._handle_error(e)
    
    def _update_progress(self, progress):
        """Update progress bar value."""
        self.progressBar['value'] = progress
        self.master.update_idletasks()
    
    def _update_progress_final(self, processed_files, total_chunks):
        """Update progress bar to final state."""
        self.progressBar['value'] = 100
        
        self.log_message(f"Processed {processed_files} files with {total_chunks} chunks")
        messagebox.showinfo("Success", "Vector store processing completed successfully!")
        self.app.update_status("Vector store processing completed successfully.")
        self.processButton.config(state=tk.NORMAL, style=self.processButtonOriginalStyle)
    
    def _handle_error(self, e):
        """Handle errors in the UI thread."""
        self.app.logger.error(f"Processing failed: {e}", exc_info=True)
        self.log_message(f"Error during processing: {e}")
        messagebox.showerror("Error", f"An error occurred during processing: {e}")
        self.app.update_status("Vector store processing failed.")
        self.processButton.config(state=tk.NORMAL, style=self.processButtonOriginalStyle)

    def saveConfig(self) -> None:
        """Save the configuration to the .env file."""
        index_path = self.indexEntry.get()
        metadata_path = self.metadataEntry.get()
        output_dir = Path(self.indexEntry.get()).parent.as_posix()
        index_name = Path(self.indexEntry.get()).name
        metadata_name = Path(self.metadataEntry.get()).name
        model_name = self.modelNameEntry.get()
        max_workers = self.maxWorkersEntry.get()
        chunk_size = self.chunkSizeEntry.get()
        chunk_overlap = self.chunkOverlapEntry.get()
        max_file_size = self.maxFileSizeEntry.get()
        disable_multiprocessing = "true" if self.disableMultiprocessingVar.get() else "false"

        dotenv_path = find_dotenv()
        if dotenv_path:
            set_key(dotenv_path, "DEFAULT_INDEX_PATH", index_path)
            set_key(dotenv_path, "DEFAULT_METADATA_PATH", metadata_path)
            set_key(dotenv_path, "DEFAULT_OUTPUT_DIR", output_dir)
            set_key(dotenv_path, "DEFAULT_INDEX_NAME", index_name)
            set_key(dotenv_path, "DEFAULT_METADATA_NAME", metadata_name)
            set_key(dotenv_path, "MODEL_NAME", model_name)
            set_key(dotenv_path, "MAX_WORKERS", max_workers)
            set_key(dotenv_path, "CHUNK_SIZE", chunk_size)
            set_key(dotenv_path, "CHUNK_OVERLAP", chunk_overlap)
            set_key(dotenv_path, "MAX_FILE_SIZE_MB", max_file_size)
            set_key(dotenv_path, "DEFAULT_INPUT_DIR", str(Path(self.inputDirEntry.get()).resolve()))
            set_key(dotenv_path, "DISABLE_MULTIPROCESSING", disable_multiprocessing)
            messagebox.showinfo("Success", "Configuration saved to .env. Please restart the application for the changes to fully take effect.")
            self.app.update_status("Configuration saved to .env")
        else:
            messagebox.showerror("Error", ".env file not found.")
            self.app.update_status("Error: .env file not found.")

## lmStudioTab.py ##

# DarthVector/lmStudioTab.py
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import requests
import json
import logging
import faiss
import pickle
from config import config
from typing import Any, List, Dict, Optional
import pyperclip
import os
from dotenv import set_key, find_dotenv
from sentence_transformers import SentenceTransformer
import gc

class LMStudioTab(ttk.Frame):
    """Tab for integrating with LM Studio."""

    def __init__(self, parent: ttk.Notebook, app: Any):
        """Initialise the LM Studio Tab."""
        super().__init__(parent)
        self.parent = parent
        self.app = app
        self.logger = logging.getLogger(__name__)
        self.defaultIndex = None
        self.vectorMetadata = []
        self.sentenceTransformerModel: Optional[SentenceTransformer] = None
        self.create_widgets()

    def create_widgets(self) -> None:
        """Create and layout widgets for the LM Studio tab."""
        # --- LM Studio API Configuration ---
        apiFrame = ttk.LabelFrame(self, text="LM Studio API Configuration")
        apiFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(apiFrame, text="API URL:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.apiUrlEntry = ttk.Entry(apiFrame, width=50)
        self.apiUrlEntry.insert(0, config.lmStudioApiUrl)
        self.apiUrlEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(apiFrame, text="Model Name:").grid(row=1, column=0, padx=5, pady=5, sticky="w")
        self.modelNameEntry = ttk.Entry(apiFrame, width=50)
        self.modelNameEntry.insert(0, config.lmStudioModelName)
        self.modelNameEntry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")

        ttk.Label(apiFrame, text="Max Tokens:").grid(row=2, column=0, padx=5, pady=5, sticky="w")
        self.maxTokensEntry = ttk.Entry(apiFrame, width=10)
        self.maxTokensEntry.insert(0, str(config.lmStudioMaxTokens))
        self.maxTokensEntry.grid(row=2, column=1, padx=5, pady=5, sticky="w")

        self.saveConfigButton = ttk.Button(apiFrame, text="Save Configuration to .env", command=self.saveLmStudioConfig)
        self.saveConfigButton.grid(row=3, column=0, columnspan=2, pady=10)

        # --- Load Vector Store Section ---
        loadFrame = ttk.LabelFrame(self, text="Vector Store")
        loadFrame.pack(padx=10, pady=10, fill="x")

        ttk.Label(loadFrame, text="Index Path:").grid(row=0, column=0, padx=5, pady=5, sticky="w")
        self.defaultIndexEntry = ttk.Entry(loadFrame, width=50)
        self.defaultIndexEntry.insert(0, str(config.defaultIndexPath))
        self.defaultIndexEntry.grid(row=0, column=1, padx=5, pady=5, sticky="ew")
        self.browsedefaultIndexButton = ttk.Button(loadFrame, text="Browse", command=self.browsedefaultIndexFile)
        self.browsedefaultIndexButton.grid(row=0, column=2, padx=5, pady=5)

        ttk.Label(loadFrame, text="Metadata Path:").grid(row=1, column=0, padx=5, pady=5, sticky="w")
        self.vectorMetadataEntry = ttk.Entry(loadFrame, width=50)
        self.vectorMetadataEntry.insert(0, str(config.defaultMetadataPath))
        self.vectorMetadataEntry.grid(row=1, column=1, padx=5, pady=5, sticky="ew")
        self.browseVectorMetadataButton = ttk.Button(loadFrame, text="Browse", command=self.browseVectorMetadataFile)
        self.browseVectorMetadataButton.grid(row=1, column=2, padx=5, pady=5)

        self.loadButton = ttk.Button(loadFrame, text="Reload Vector Store", command=self.loadVectorStore)
        self.loadButton.grid(row=2, column=0, columnspan=3, pady=5)

        # --- Query Section ---
        queryFrame = ttk.LabelFrame(self, text="Query Vector Store with LM Studio")
        queryFrame.pack(padx=10, pady=10, fill="both", expand=True)

        ttk.Label(queryFrame, text="Your Query:").pack(padx=5, pady=5, anchor="w")
        self.queryEntry = ttk.Entry(queryFrame, width=60)
        self.queryEntry.pack(padx=5, pady=5, fill="x")

        ttk.Label(queryFrame, text=f"Number of Search Results (Top {config.searchResultCount}):").pack(padx=5, pady=5, anchor="w")
        self.searchResultsText = scrolledtext.ScrolledText(queryFrame, height=5, wrap=tk.WORD)
        self.searchResultsText.pack(padx=5, pady=5, fill="both", expand=True)
        self.searchResultsText.config(state=tk.DISABLED)

        self.queryButton = ttk.Button(queryFrame, text="Query", command=self.executeQuery)
        self.queryButton.pack(pady=10)

        # --- Response Section ---
        responseFrame = ttk.LabelFrame(self, text="LM Studio Response")
        responseFrame.pack(padx=10, pady=10, fill="both", expand=True)

        self.responseArea = scrolledtext.ScrolledText(responseFrame, height=15, wrap=tk.WORD)
        self.responseArea.pack(padx=5, pady=5, fill="both", expand=True)
        self.responseArea.config(state=tk.DISABLED)

        # Add a copy button for the response
        self.copyResponseButton = ttk.Button(responseFrame, text="Copy Response", command=self.copyResponse)
        self.copyResponseButton.pack(pady=5)

    def copyResponse(self) -> None:
        """Copy the response text to clipboard."""
        response_text = self.responseArea.get(1.0, tk.END).strip()
        if response_text:
            pyperclip.copy(response_text)
            self.app.update_status("Response copied to clipboard")
        else:
            self.app.update_status("No response to copy")

    def browsedefaultIndexFile(self) -> None:
        """Open file selection dialogue for the FAISS index file."""
        filePath = filedialog.askopenfilename(defaultextension=".faiss", filetypes=[("FAISS Index Files", "*.faiss"), ("All Files", "*.*")])
        if filePath:
            self.defaultIndexEntry.delete(0, tk.END)
            self.defaultIndexEntry.insert(0, filePath)

    def browseVectorMetadataFile(self) -> None:
        """Open file selection dialogue for the metadata file."""
        filePath = filedialog.askopenfilename(defaultextension=".pkl", filetypes=[("Pickle Files", "*.pkl"), ("All Files", "*.*")])
        if filePath:
            self.vectorMetadataEntry.delete(0, tk.END)
            self.vectorMetadataEntry.insert(0, filePath)

    def loadVectorStore(self) -> None:
        """Load the FAISS vector store and metadata."""
        indexFile = self.defaultIndexEntry.get()
        metadataFile = self.vectorMetadataEntry.get()
        
        # Create directories if they don't exist
        os.makedirs(os.path.dirname(os.path.abspath(indexFile)), exist_ok=True)
        os.makedirs(os.path.dirname(os.path.abspath(metadataFile)), exist_ok=True)
        
        try:
            # Load sentence transformer model first to get dimensions
            if not hasattr(self.app, 'sentenceTransformerModel') or self.app.sentenceTransformerModel is None:
                try:
                    self.sentenceTransformerModel = SentenceTransformer(config.defaultModelName)
                    self.app.sentenceTransformerModel = self.sentenceTransformerModel
                    model_dimension = self.sentenceTransformerModel.get_sentence_embedding_dimension()
                    self.app.update_status(f"Model loaded with dimension: {model_dimension}")
                except Exception as e:
                    messagebox.showerror("Error", f"Error loading SentenceTransformer model: {e}")
                    self.logger.error(f"Error loading SentenceTransformer model: {e}", exc_info=True)
                    return
            
            # Now that we have the model dimension, handle the index file
            if not os.path.exists(indexFile):
                # Create an empty FAISS index with the correct dimension from the model
                model_dimension = self.sentenceTransformerModel.get_sentence_embedding_dimension()
                empty_index = faiss.IndexFlatL2(model_dimension)
                faiss.write_index(empty_index, str(indexFile))
                self.logger.info(f"Created new FAISS index with dimension {model_dimension}")
                messagebox.showinfo("Info", f"Created new FAISS index with dimension {model_dimension}")
                
            # Load the vector index
            self.defaultIndex = faiss.read_index(str(indexFile))
            
            # Check dimension compatibility
            model_dimension = self.sentenceTransformerModel.get_sentence_embedding_dimension()
            index_dimension = self.defaultIndex.d
            
            if model_dimension != index_dimension:
                messagebox.showerror("Error", 
                    f"Dimension mismatch: model dimension ({model_dimension}) "
                    f"doesn't match index dimension ({index_dimension}). "
                    f"Use a compatible model or recreate the vector store.")
                del self.defaultIndex
                self.defaultIndex = None
                self.vectorMetadata = []
                return
                
            # Load or create the metadata file
            if not os.path.exists(metadataFile):
                with open(metadataFile, "wb") as f:
                    pickle.dump([], f)
                self.logger.info(f"Created empty metadata file at {metadataFile}")
                
            with open(metadataFile, "rb") as file:
                self.vectorMetadata = pickle.load(file)
                
            messagebox.showinfo("Success", "Vector store and metadata loaded successfully.")
            self.app.update_status("Vector store and metadata loaded successfully.")
            
        except FileNotFoundError as e:
            messagebox.showerror("Error", f"File not found: {e}")
            self.app.update_status(f"Error: {e}")
            self.defaultIndex = None
            self.vectorMetadata = []
            gc.collect()
        except Exception as e:
            messagebox.showerror("Error", f"Error loading vector store: {e}")
            self.app.update_status(f"Error loading vector store: {e}")
            self.logger.error(f"Error loading vector store: {e}", exc_info=True)
            if self.defaultIndex is not None:
                del self.defaultIndex
            self.defaultIndex = None
            self.vectorMetadata = []
            gc.collect()
 
    def executeQuery(self) -> None:
        """Execute the query against LM Studio with context from the vector store."""
        query = self.queryEntry.get()
        if not query:
            messagebox.showerror("Error", "Please enter a query.")
            return
            
        # Check if vector store is loaded
        if self.defaultIndex is None or not self.vectorMetadata:
            messagebox.showerror("Error", "Vector store not loaded. Please load the vector store first.")
            return
            
        # Check if sentence transformer model is available
        if not hasattr(self.app, 'sentenceTransformerModel') or self.app.sentenceTransformerModel is None:
            if not hasattr(self, 'sentenceTransformerModel') or self.sentenceTransformerModel is None:
                messagebox.showerror("Error", "SentenceTransformer model not loaded.")
                return
            self.app.sentenceTransformerModel = self.sentenceTransformerModel

        self.app.update_status("Searching vector store...")
        searchResults = self.performVectorSearch(query, top_n=config.searchResultCount)
        self.displaySearchResults(searchResults)
        
        if not searchResults:
            messagebox.showinfo("Information", "No relevant search results found.")
            return
            
        context = "\n\n".join([result['metadata']['chunkText'] for result in searchResults])

        prompt = f"""Based on the following context:
                {context}
                
                Answer the query: {query}"""

        self.app.update_status("Querying LM Studio...")
        try:
            max_tokens = int(self.maxTokensEntry.get())
            if max_tokens <= 0:
                raise ValueError("Max Tokens must be positive")
        except ValueError as e:
            messagebox.showerror("Error", f"Invalid Max Tokens value: {e}")
            self.app.update_status("Error: Invalid Max Tokens value")
            return
        self.queryLMStudio(prompt, max_tokens)

    def performVectorSearch(self, query: str, top_n: int = 5) -> List[Dict]:
        """Perform a vector similarity search."""
        if self.defaultIndex is None:
            messagebox.showerror("Error", "Vector index not loaded. Please load the vector store first.")
            return []
        if not self.vectorMetadata:
            messagebox.showerror("Error", "Vector metadata is empty.")
            return []

        try:
            # Use either the app's model or this tab's model
            model = getattr(self.app, 'sentenceTransformerModel', None) or self.sentenceTransformerModel
            if model is None:
                raise ValueError("SentenceTransformer model not available")
                
            queryEmbedding = model.encode([query])[0]
            distances, indices = self.defaultIndex.search(queryEmbedding.reshape(1, -1), top_n)

            results = []
            for i in range(len(indices[0])):
                index = indices[0][i]
                if index != -1 and index < len(self.vectorMetadata):
                    results.append({
                        'metadata': self.vectorMetadata[index],
                        'distance': float(distances[0][i])
                    })
            return results
        except Exception as e:
            self.logger.error(f"Error performing vector search: {e}", exc_info=True)
            messagebox.showerror("Error", f"Error performing vector search: {e}")
            return []

    def displaySearchResults(self, searchResults: List[Dict]) -> None:
        """Display the search results in the text area."""
        self.searchResultsText.config(state=tk.NORMAL)
        self.searchResultsText.delete(1.0, tk.END)
        if searchResults:
            for result in searchResults:
                metadata = result['metadata']
                text = metadata.get('chunkText', 'No text available')
                filename = metadata.get('file', 'Unknown file')
                chunkIndex = metadata.get('chunkIndex', 'Unknown index')
                distance = result.get('distance', 0)
                self.searchResultsText.insert(tk.END, f"File: {filename}, Chunk: {chunkIndex}, Distance: {distance:.4f}\n{text}\n---\n")
        else:
            self.searchResultsText.insert(tk.END, "No relevant search results found.\n")
        self.searchResultsText.config(state=tk.DISABLED)

    def queryLMStudio(self, prompt: str, max_tokens: int) -> None:
        """Send the prompt to the LM Studio API and display the response."""
        apiUrl = self.apiUrlEntry.get()
        # Normalize apiUrl to ensure correct base URL
        base_url = apiUrl.rstrip('/').split('/v1')[0]
        endpoint = f"{base_url}/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": self.modelNameEntry.get(),
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "stream": False
        }

        try:
            response = requests.post(endpoint, headers=headers, data=json.dumps(data))
            response.raise_for_status()
            responseJson = response.json()
            if 'choices' in responseJson and responseJson['choices']:
                assistantResponse = responseJson['choices'][0]['message']['content']
                self.displayResponse(assistantResponse)
            else:
                self.displayResponse("No response received from LM Studio or unexpected format.")
                self.logger.warning(f"Unexpected response format from LM Studio: {responseJson}")
        except requests.exceptions.ConnectionError as e:
            self.displayResponse(f"Error: Could not connect to LM Studio at {endpoint}. Please ensure LM Studio is running and the API URL is correct.")
            self.logger.error(f"Connection error to LM Studio at {endpoint}: {e}")
        except requests.exceptions.RequestException as e:
            self.displayResponse(f"Error querying LM Studio: {e}")
            self.logger.error(f"Error querying LM Studio: {e}")
        except json.JSONDecodeError:
            self.displayResponse("Error: Could not decode JSON response from LM Studio.")
            self.logger.error("Could not decode JSON response from LM Studio.")

    def displayResponse(self, response: str) -> None:
        """Display the LM Studio response in the response area."""
        self.responseArea.config(state=tk.NORMAL)
        self.responseArea.delete(1.0, tk.END)
        self.responseArea.insert(tk.END, response)
        self.responseArea.config(state=tk.DISABLED)
        self.app.update_status("Ready")

    def saveLmStudioConfig(self) -> None:
        """Save the LM Studio API URL, Model Name, and Max Tokens to the .env file."""
        api_url = self.apiUrlEntry.get()
        model_name = self.modelNameEntry.get()
        max_tokens = self.maxTokensEntry.get()
        dotenv_path = find_dotenv()
        if dotenv_path:
            set_key(dotenv_path, "LMSTUDIO_API_URL", api_url)
            set_key(dotenv_path, "LMSTUDIO_MODEL_NAME", model_name)
            set_key(dotenv_path, "LMSTUDIO_MAX_TOKENS", max_tokens)
            messagebox.showinfo("Success", "LM Studio configuration saved to .env. Please restart the application for the changes to fully take effect.")
            self.app.update_status("LM Studio configuration saved to .env")
        else:
            messagebox.showerror("Error", ".env file not found.")
            self.app.update_status("Error: .env file not found.")

## main.py ##

import argparse
import logging
import sys
import time
import multiprocessing as mp
import torch.multiprocessing as mp
from pathlib import Path
from tqdm import tqdm
from typing import Any, Dict, List, Optional, Tuple
import json
import os
import numpy as np
import gc

from config import config
from textExtractor import extractTextFromFile, FileProcessingError
from exceptions import FileSizeExceededError, UnsupportedFileTypeError
from textPreprocessor import TextPreprocessor
from embeddingGenerator import SentenceTransformerEmbedder
from vectorStore import VectorStore

# Set the multiprocessing start method globally
def initialize_multiprocessing():
    """Initialize multiprocessing with appropriate method."""
    # 'spawn' is default on Windows, we want to use it on all platforms for consistency
    if sys.platform != 'win32' and mp.get_start_method(allow_none=True) is None:
        try:
            mp.set_start_method('spawn')
            logging.info("Multiprocessing start method set to 'spawn'")
        except RuntimeError:
            logging.warning("Multiprocessing start method already set, using existing configuration")

# Initialize multiprocessing at module import time
initialize_multiprocessing()

class RtfProcessingPipeline:
    """Pipeline for processing documents into embeddings."""

    def __init__(
        self,
        modelName: Optional[str] = None,
        chunkSize: Optional[int] = None,
        chunkOverlap: Optional[int] = None,
        maxWorkers: Optional[int] = None,
        maxFileSizeMB: Optional[int] = None,
        useMultiprocessing: Optional[bool] = None,
        embeddingDimension: Optional[int] = None,
        ingestedFilesPath: Optional[str] = None,
        existingVectorStore: Optional[VectorStore] = None
    ) -> None:
        """
        Initialise the pipeline.

        Args:
            modelName: SentenceTransformer model name.
            chunkSize: Maximum words per chunk.
            chunkOverlap: Overlap between chunks.
            maxWorkers: Number of worker processes.
            maxFileSizeMB: Maximum file size in MB.
            useMultiprocessing: Whether to use multiprocessing.
            embeddingDimension: Dimension of the embeddings.
            ingestedFilesPath: Path to the file storing the list of ingested files.
            existingVectorStore: An existing VectorStore instance to use.
        """
        self.modelName = modelName or config.defaultModelName
        self.chunkSize = chunkSize or config.defaultChunkSize
        self.chunkOverlap = chunkOverlap or config.defaultChunkOverlap
        self.maxWorkers = maxWorkers or config.defaultMaxWorkers
        self.maxFileSizeMB = maxFileSizeMB or config.maxFileSizeMB
        
        if useMultiprocessing is not None:
            self.useMultiprocessing = useMultiprocessing
        else:
            self.useMultiprocessing = not config.disableMultiprocessing
            
        # Initialize embedder first to get dimensions if needed
        self.embedder = SentenceTransformerEmbedder(self.modelName)
        
        # Use provided dimension or get from model
        self.embeddingDimension = embeddingDimension or self.embedder.embeddingDim
        
        # Use existing vector store or create new one
        if existingVectorStore:
            self.vectorStore = existingVectorStore
        else:
            self.vectorStore = VectorStore(embeddingDimension=self.embeddingDimension, modelName=self.modelName)
            
        self.ingestedFilesPath = ingestedFilesPath or config.defaultIngestedFilesPath
        self.ingestedFiles = self._loadIngestedFiles() or []  # Default to empty list if None
        
        logging.info(f"Pipeline initialised with model: {self.modelName}, multiprocessing: {self.useMultiprocessing}")
    
    def _loadIngestedFiles(self) -> List[Dict[str, Any]]:
        """Load the list of ingested files (name and size) from disk."""
        try:
            ingestedFilesDir = Path(self.ingestedFilesPath).parent
            ingestedFilesDir.mkdir(exist_ok=True, parents=True)
            
            if not Path(self.ingestedFilesPath).exists():
                with open(self.ingestedFilesPath, 'w') as f:
                    json.dump([], f)
                return []
                
            with open(self.ingestedFilesPath, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            logging.info(f"Ingested files list not found at {self.ingestedFilesPath}. Creating an empty file.")
            os.makedirs(os.path.dirname(os.path.abspath(self.ingestedFilesPath)), exist_ok=True)
            with open(self.ingestedFilesPath, 'w') as f:
                json.dump([], f)
            return []
        except json.JSONDecodeError:
            logging.warning("Could not decode ingested files list, starting with an empty list.")
            return []
        except Exception as e:
            logging.error(f"Error loading ingested files list: {e}")
            return []
    
    def _saveIngestedFiles(self) -> None:
        """Save the list of ingested files (name and size) to disk atomically."""
        # Use atomic write pattern with temporary file
        temp_path = Path(self.ingestedFilesPath).with_name(f"{Path(self.ingestedFilesPath).name}.tmp")
        try:
            with open(temp_path, 'w') as f:
                json.dump(self.ingestedFiles, f)
            
            # Atomic replacement
            os.replace(temp_path, self.ingestedFilesPath)
            logging.info(f"Saved ingested files list to {self.ingestedFilesPath}")
        except Exception as e:
            # Clean up temp file
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
            logging.error(f"Error saving ingested files list: {e}")

    def processFile(self, filePath: Path) -> int:
        """
        Process a single file and add its embeddings to the vector store.
        
        Args:
            filePath: Path to the file to process
            
        Returns:
            Number of chunks processed
        """
        try:
            # Check if file already processed
            fileName = filePath.name
            fileSize = os.path.getsize(filePath)
            if any(ingested_file.get('name') == fileName and 
                   ingested_file.get('size') == fileSize 
                   for ingested_file in self.ingestedFiles):
                logging.info(f"File already ingested: {fileName} (size: {fileSize} bytes)")
                return 0
                
            # Extract and process text
            startTime = time.time()
            rawText = extractTextFromFile(filePath)
            
            textPreprocessor = TextPreprocessor(self.chunkSize, self.chunkOverlap)
            cleanedText = textPreprocessor.cleanText(rawText)
            chunks = textPreprocessor.segmentText(cleanedText)
            
            if not chunks:
                logging.warning(f"No chunks from {filePath}")
                return 0
                
            # Generate embeddings
            embeddings = self.embedder.encodeChunks(chunks)
            
            # Create metadata
            metadatas = [
                {"file": filePath.name, "chunkIndex": idx, "chunkLength": len(chunk.split()), "chunkText": chunk}
                for idx, chunk in enumerate(chunks)]
                
            # Add to vector store
            self.vectorStore.addEmbeddings(embeddings, metadatas)
            
            # Update ingested files list
            self.ingestedFiles.append({'name': fileName, 'size': fileSize})
            self._saveIngestedFiles()
            
            processingTime = time.time() - startTime
            logging.info(f"Processed {filePath} in {processingTime:.2f}s: {len(chunks)} chunks")
            return len(chunks)
            
        except (FileProcessingError, FileSizeExceededError, UnsupportedFileTypeError) as e:
            logging.error(f"Processing error for {filePath}: {e}")
            return 0
        except Exception as e:
            logging.error(f"Unexpected error for {filePath}: {e}", exc_info=True)
            return 0

    def processDirectory(self, inputDir: Path, progress_callback=None) -> int:
        """
    Process new files in a directory and subdirectories, skipping already ingested ones.
        
        Args:
            inputDir: Directory containing files to process
            progress_callback: Optional callback function to report progress
            
        Returns the number of successfully processed files.
        """
        resolvedInputDir = inputDir.resolve()
        if not resolvedInputDir.is_dir():
            raise ValueError(f"Invalid directory: {resolvedInputDir}")

    # Find files with supported extensions recursively
        supportedFiles = []
        for ext in config.supportedExtensions:
            # Use ** pattern for recursive search
            supportedFiles.extend(list(resolvedInputDir.glob(f"**/*{ext}")))
        
        supportedFiles = sorted(supportedFiles)
        if not supportedFiles:
            logging.warning(f"No supported files in {resolvedInputDir}")
            return 0

        # Build list of files that need processing by checking against ingested files
        filesToProcess = []
        for filePath in supportedFiles:
            fileName = filePath.name
            fileSize = os.path.getsize(filePath)
            
            # Skip if already ingested
            if any(ingested_file.get('name') == fileName and 
                   ingested_file.get('size') == fileSize 
                   for ingested_file in self.ingestedFiles):
                logging.info(f"File already ingested: {fileName} (size: {fileSize} bytes)")
                continue
                
            filesToProcess.append(filePath)

        if not filesToProcess:
            logging.info("All files already processed")
            return 0
            
        logging.info(f"Processing {len(filesToProcess)} files, skipping {len(supportedFiles) - len(filesToProcess)} already ingested files")
        
        successfulFiles = 0
        totalChunks = 0

        # Process files using the selected method (multiprocessing or sequential)
        if self.useMultiprocessing and self.maxWorkers > 1:
            # Group files in batches to reduce model loading overhead
            batch_size = max(1, len(filesToProcess) // self.maxWorkers)
            file_batches = [filesToProcess[i:i + batch_size] for i in range(0, len(filesToProcess), batch_size)]
            
            # Create a process pool safely using spawn context
            ctx = mp.get_context('spawn') 
            
            # Use context manager to ensure proper cleanup
            with ctx.Pool(processes=min(self.maxWorkers, len(file_batches))) as pool:
                try:
                    # Process batches in parallel
                    batch_results = pool.map(
                        self._process_file_batch,
                        [(batch, self.maxFileSizeMB, self.chunkSize, self.chunkOverlap, self.modelName) 
                         for batch in file_batches]
                    )
                    
                    # Handle results from all batches
                    for batch_result in batch_results:
                        for filePath, (embeddings, metadatas, chunksProcessed) in batch_result:
                            if embeddings is not None and metadatas is not None and chunksProcessed > 0:
                                self.vectorStore.addEmbeddings(embeddings, metadatas)
                                successfulFiles += 1
                                totalChunks += chunksProcessed
                                
                                fileName = filePath.name
                                fileSize = os.path.getsize(filePath)
                                self.ingestedFiles.append({'name': fileName, 'size': fileSize})
                                self._saveIngestedFiles()
                                
                                logging.info(f"Successfully processed file: {fileName}, {chunksProcessed} chunks.")
                                
                                # Update progress if callback provided
                                if progress_callback:
                                    progress_callback(chunksProcessed)
                                    
                except Exception as e:
                    logging.error(f"Error in parallel processing: {e}", exc_info=True)
        else:
            # Sequential processing
            for filePath in filesToProcess:
                try:
                    fileName = filePath.name
                    fileSize = os.path.getsize(filePath)
                    
                    # Extract text
                    rawText = extractTextFromFile(filePath)
                    textPreprocessor = TextPreprocessor(self.chunkSize, self.chunkOverlap)
                    cleanedText = textPreprocessor.cleanText(rawText)
                    chunks = textPreprocessor.segmentText(cleanedText)
                    
                    if not chunks:
                        logging.warning(f"No chunks from {filePath}")
                        continue
                        
                    # Generate embeddings
                    embeddings = self.embedder.encodeChunks(chunks)
                    
                    # Create metadata
                    metadatas = [
                        {"file": fileName, "chunkIndex": idx, "chunkLength": len(chunk.split()), "chunkText": chunk}
                        for idx, chunk in enumerate(chunks)]
                        
                    # Add to vector store
                    self.vectorStore.addEmbeddings(embeddings, metadatas)
                    
                    # Update ingested files list
                    self.ingestedFiles.append({'name': fileName, 'size': fileSize})
                    self._saveIngestedFiles()
                    
                    successfulFiles += 1
                    totalChunks += len(chunks)
                    
                    logging.info(f"Successfully processed file: {fileName}, {len(chunks)} chunks.")
                    
                    # Update progress if callback provided
                    if progress_callback:
                        progress_callback(1)  # Increment by 1 file
                        
                except Exception as e:
                    logging.error(f"Error processing file {filePath}: {e}", exc_info=True)
                    # Continue processing other files

        logging.info(f"Processed {successfulFiles}/{len(filesToProcess)} files, {totalChunks} chunks")
        return successfulFiles

    @staticmethod
    def _process_file_batch(args):
        """Process a batch of files with a single model instance."""
        batch, maxFileSizeMB, chunkSize, chunkOverlap, modelName = args
        results = []
        embedder = None
        try:
            # Load model once for the batch
            embedder = SentenceTransformerEmbedder(modelName)
            for filePath in batch:
                try:
                    # Extract text
                    rawText = extractTextFromFile(filePath)
                    textPreprocessor = TextPreprocessor(chunkSize, chunkOverlap)
                    cleanedText = textPreprocessor.cleanText(rawText)
                    chunks = textPreprocessor.segmentText(cleanedText)
                    
                    if not chunks:
                        logging.warning(f"No chunks from {filePath}")
                        results.append((filePath, (None, None, 0)))
                        continue
                    
                    # Generate embeddings reusing the same embedder
                    embeddings = embedder.encodeChunks(chunks)
                    
                    # Create metadata
                    metadatas = [
                        {"file": filePath.name, "chunkIndex": idx, "chunkLength": len(chunk.split()), "chunkText": chunk}
                        for idx, chunk in enumerate(chunks)
                    ]
                    
                    results.append((filePath, (embeddings, metadatas, len(chunks))))
                except Exception as e:
                    logging.error(f"Error processing {filePath}: {e}")
                    results.append((filePath, (None, None, 0)))
        finally:
            # Clean up resources
            if embedder:
                del embedder
            gc.collect()
        return results

    def saveResults(self, indexPath: Optional[str] = None, metadataPath: Optional[str] = None) -> None:
        """Save the vector store to disk."""
        try:
            self.vectorStore.save(indexPath, metadataPath)
            logging.info("Vector store saved successfully.")
            self.vectorStore.release_model() # Release the model after saving
        except Exception as e:
            logging.error(f"Error during saving vector store: {e}", exc_info=True)
            raise  # Propagate the error

def parseArguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Process RTF files into embeddings for RAG.")
    parser.add_argument("--input_dir", type=str, required=True, help="Directory with RTF files")
    parser.add_argument("--output_index", type=str, default=str(config.defaultIndexPath), help="FAISS index path")
    parser.add_argument("--output_metadata", type=str, default=str(config.defaultMetadataPath), help="Metadata path")
    parser.add_argument("--model_name", type=str, default=config.defaultModelName, help="SentenceTransformer model")
    parser.add_argument("--max_workers", type=int, default=config.defaultMaxWorkers, help="Number of workers")
    parser.add_argument("--chunk_size", type=int, default=config.defaultChunkSize, help="Words per chunk")
    parser.add_argument("--chunk_overlap", type=int, default=config.defaultChunkOverlap, help="Chunk overlap")
    parser.add_argument("--max_file_size_mb", type=int, default=config.maxFileSizeMB, help="Maximum file size in MB")
    parser.add_argument("--disable_multiprocessing", action='store_true', help="Disable multiprocessing")
    parser.add_argument("--ingested_files_path", type=str, default=str(config.defaultIngestedFilesPath), help="Path to the ingested files list")
    return parser.parse_args()

def main() -> None:
    """Main entry point."""
    # Configure logging
    logging.basicConfig(
        level=getattr(logging, config.loggingLevel),
        format=config.loggingFormat,
        handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler("rtf_processor.log")]
    )

    # Parse arguments
    args = parseArguments()

    try:
        # Create output directories if they don't exist
        os.makedirs(os.path.dirname(os.path.abspath(args.output_index)), exist_ok=True)
        os.makedirs(os.path.dirname(os.path.abspath(args.output_metadata)), exist_ok=True)
        
        # Create pipeline
        pipeline = RtfProcessingPipeline(
            modelName=args.model_name,
            chunkSize=args.chunk_size,
            chunkOverlap=args.chunk_overlap,
            maxWorkers=args.max_workers,
            maxFileSizeMB=args.max_file_size_mb,
            useMultiprocessing=not args.disable_multiprocessing,
            ingestedFilesPath=args.ingested_files_path
        )

        # Process files
        processedFiles = pipeline.processDirectory(Path(args.input_dir))
        
        # Save results
        pipeline.saveResults(args.output_index, args.output_metadata)

        logging.info(f"Processing complete. {processedFiles} files processed successfully.")
        logging.info(f"Vector store saved to {args.output_index} and {args.output_metadata}")

    except Exception as e:
        logging.error(f"Pipeline failed: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()

## textPreprocessor.py ##

# DarthVector/textPreprocessor.py
# ---------------------------------
# File: textPreprocessor.py
# ---------------------------------
import re
import logging
from typing import List

class TextPreprocessor:
    """
    Class for preprocessing text by cleaning, tokenizing, chunking, and splitting into sentences.
    """

    def __init__(self, chunkSize: int = 500, chunkOverlap: int = 50) -> None:
        """
        Initialise the TextPreprocessor with chunking parameters.

        Args:
            chunkSize: Maximum number of words per text chunk (default: 500).
            chunkOverlap: Number of overlapping words between consecutive chunks (default: 50).
            
        Raises:
            ValueError: If chunk parameters are invalid.
        """
        self.logger = logging.getLogger(__name__)
        
        if chunkSize <= 0:
            raise ValueError("chunkSize must be positive")
        if chunkOverlap < 0:
            raise ValueError("chunkOverlap must be non-negative")
        if chunkOverlap >= chunkSize:
            raise ValueError("chunkOverlap must be less than chunkSize")
            
        self.chunkSize = chunkSize
        self.chunkOverlap = chunkOverlap

    def cleanText(self, text: str, preservePunctuation: bool = False) -> str:
        """
        Clean the input text by removing extra whitespace and normalizing.

        Args:
            text: The input string to be cleaned.
            preservePunctuation: If True, preserve punctuation marks (default: False).

        Returns:
            A cleaned version of the input string.
        """
        try:
            text = re.sub(r'\s+', ' ', text.strip())
            if not preservePunctuation:
                text = re.sub(r'[^\w\s\-.,;:!?\'"\(\)]', '', text)
            return text
        except Exception as e:
            self.logger.error(f"Error cleaning text: {e}")
            raise

    def tokenizeText(self, text: str) -> List[str]:
        """
        Tokenize the text into words.

        Args:
            text: The input string to tokenize.

        Returns:
            A list of word tokens.
        """
        try:
            tokens = re.findall(r"\b[\w']+(?:-[\w']+)*\b", text.lower())
            return tokens
        except Exception as e:
            self.logger.error(f"Error tokenizing text: {e}")
            raise

    def segmentText(self, text: str) -> List[str]:
        """
        Segment text into chunks, treating hyphenated words and apostrophes correctly.

        Args:
            text: The input string to be segmented.

        Returns:
            A list of text chunks.
        """
        try:
            clean_text = self.cleanText(text, preservePunctuation=True)
            tokens = self.tokenizeText(clean_text)
        
            if not tokens:
                return []
                
            chunks = []
            stride = max(1, self.chunkSize - self.chunkOverlap)
            
            for i in range(0, len(tokens), stride):
                chunk = tokens[i:i + self.chunkSize]
                if chunk:
                    chunks.append(" ".join(chunk))
                    
            return chunks
        except Exception as e:
            self.logger.error(f"Error segmenting text: {e}")
            raise

    def splitSentences(self, text: str) -> List[str]:
        """
        Split text into sentences, handling common abbreviations.

        Args:
            text: The input text to split.

        Returns:
            A list of sentences.
        """
        try:
            clean_text = self.cleanText(text, preservePunctuation=True)
            pattern = r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=[.!?])\s'
            sentences = re.split(pattern, clean_text)
            return [s.strip() for s in sentences if s.strip()]
        except Exception as e:
            self.logger.error(f"Error splitting sentences: {e}")
            raise

    def preprocess(self, text: str) -> str:
        """
        Preprocess the text by cleaning it (basic entry point).

        Args:
            text: The input string to preprocess.

        Returns:
            A cleaned version of the input string.
        """
        try:
            return self.cleanText(text)
        except Exception as e:
            self.logger.error(f"Error preprocessing text: {e}")
            raise

## exceptions.py ##

"""
Common exceptions for the RTF Processing Pipeline.
Centralizes exception definitions to avoid duplication.
"""

class FileProcessingError(Exception):
    """Base exception for file processing errors."""
    pass

class TextExtractionError(FileProcessingError):
    """Exception for text extraction errors."""
    pass

class FileSizeExceededError(FileProcessingError):
    """Exception when file size exceeds the limit."""
    pass

class UnsupportedFileTypeError(FileProcessingError):
    """Exception for unsupported file types."""
    pass

class RTFEncodingError(FileProcessingError):
    """Exception raised for RTF encoding issues."""
    pass

## rtfProcessor.py ##

# ---------------------------
# File: rtfProcessor.py
# ---------------------------
import logging
from pathlib import Path
from striprtf.striprtf import rtf_to_text
from typing import Union
from exceptions import FileProcessingError, TextExtractionError, FileSizeExceededError, UnsupportedFileTypeError, RTFEncodingError
from config import config

class RtfProcessor:
    """Processes RTF files to extract plain text."""

    def __init__(self):
        """Initialise the RtfProcessor."""
        self.logger = logging.getLogger(__name__)
        self.encodings = getattr(config, 'rtfEncodings', ['utf-8', 'latin-1', 'cp1252', 'utf-16', 'utf-16-le'])

    def extractTextFromRtf(self, filePath: Union[str, Path]) -> str:
        """
        Extract plain text from an RTF file.

        Args:
            filePath: Path to the RTF document (str or Path).

        Returns:
            str: Extracted plain text.

        Raises:
            FileNotFoundError: If the file doesn't exist.
            FileSizeExceededError: If file size exceeds the limit.
            RTFEncodingError: If encoding cannot be determined.
            FileProcessingError: For other processing errors.
        """
        try:
            path = Path(filePath).resolve()  # Prevent path traversal
            self.logger.debug(f"Processing file: {path}")
            if not path.exists():
                raise FileNotFoundError(f"File not found: {path}")

            fileSizeMb = path.stat().st_size / (1024 * 1024)
            self.logger.debug(f"File size: {fileSizeMb:.2f}MB for {path}")
            if fileSizeMb > config.maxFileSizeMB:
                raise FileSizeExceededError(
                f"File size ({fileSizeMb:.2f}MB) exceeds limit ({config.maxFileSizeMB}MB)"
                )

            self.logger.debug(f"Reading RTF file: {path}")
            rtfContent = None
            decode_errors = []
        
            for encoding in self.encodings:
                try:
                    rtfContent = path.read_text(encoding=encoding)
                    self.logger.debug(f"Successfully decoded using {encoding} encoding")
                    break
                except UnicodeDecodeError as e:
                    decode_errors.append(f"{encoding}: {str(e)}")
                    self.logger.debug(f"Failed to decode RTF with {encoding}: {e}")
                    continue
                    
            if rtfContent is None:
                    error_msg = f"Failed to decode RTF file {path} with encodings: {', '.join(self.encodings)}"
                    self.logger.error(error_msg)
                    self.logger.debug(f"Decode errors: {decode_errors}")
                    raise RTFEncodingError(error_msg)

            plainText = rtf_to_text(rtfContent)
            if not plainText.strip():
                self.logger.warning(f"RTF content from {path} resulted in empty text.")
                self.logger.info(f"Extracted {len(plainText)} characters from {path}")
                return plainText

        except FileNotFoundError as e:
            self.logger.error(f"Error processing {filePath}: {e}")
            raise
        except FileSizeExceededError as e:
            self.logger.error(f"Error processing {filePath}: {e}")
            raise
        except RTFEncodingError as e:
            self.logger.error(str(e))
            raise
        except Exception as e:
            self.logger.error(f"Failed to process {path}: {e}")
            raise FileProcessingError(f"Error processing {path}: {str(e)}")

## gui.py ##

import tkinter as tk
from tkinter import ttk
from pathlib import Path
import logging
import sys
import faiss
import pickle
import atexit
from sentence_transformers import SentenceTransformer
from config import config
from fileProcessingTab import FileProcessingTab
from updateVectorStoreTab import UpdateVectorStoreTab
from lmStudioTab import LMStudioTab
from embeddingGenerator import SentenceTransformerEmbedder
import gc

class RtfProcessingApp:
    """A modern GUI for processing RTF files into embeddings."""

    def __init__(self, master: tk.Tk):
        """Initialise the main application window."""
        self.master = master
        self.master.title("RTF Processing Pipeline")
        self.totalFiles = 0
        
        # Don't load model at initialization - load on demand instead
        self.sentenceTransformerModel = None

        # Ensure vector and metadata files exist
        self.defaultIndexPath = Path(config.defaultIndexPath)
        self.defaultMetadataPath = Path(config.defaultMetadataPath)
        self.ensure_files_exist()

        # Configure logging
        logging.basicConfig(
            level=getattr(logging, config.loggingLevel),
            format=config.loggingFormat,
            handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler("gui.log")]
        )
        self.logger = logging.getLogger(__name__)

        # Style Configuration
        self.style = ttk.Style()
        self.style.theme_use('clam')

        self.defaultIndex = None
        self.vectorMetadata = []
        self.create_widgets()

        # Status Bar
        self.statusBarText = tk.StringVar()
        self.statusBar = ttk.Label(master, textvariable=self.statusBarText, relief=tk.SUNKEN, anchor=tk.W)
        self.statusBar.pack(side=tk.BOTTOM, fill=tk.X)
        self.update_status("Ready")

        # Set initial full screen
        self.master.attributes('-fullscreen', True)

        # Add menu bar for full-screen toggle
        self.menu = tk.Menu(self.master)
        self.master.config(menu=self.menu)
        view_menu = tk.Menu(self.menu, tearoff=0)
        self.menu.add_cascade(label="View", menu=view_menu)
        view_menu.add_command(label="Toggle Full Screen", command=self.toggle_fullscreen)

        # Bind F11 to toggle full screen
        self.master.bind("<F11>", lambda event: self.toggle_fullscreen())
        
        # Register cleanup handler when app closes
        atexit.register(self.cleanup_resources)
        self.master.protocol("WM_DELETE_WINDOW", self.on_closing)

        # Load vector store after full initialization
        self.lmStudioTab.loadVectorStore()

    def get_model(self):
        """Get or load the sentence transformer model on demand"""
        if self.sentenceTransformerModel is None:
            self.update_status("Loading sentence transformer model...")
            self.sentenceTransformerModel = SentenceTransformer(config.defaultModelName)
            self.update_status("Model loaded")
        return self.sentenceTransformerModel

    def toggle_fullscreen(self):
        """Toggle between full-screen and windowed mode."""
        current = self.master.attributes('-fullscreen')
        self.master.attributes('-fullscreen', not current)

    def ensure_files_exist(self) -> None:
        """Ensure vector and metadata files are created if missing."""
        self.defaultIndexPath.parent.mkdir(parents=True, exist_ok=True)
        self.defaultMetadataPath.parent.mkdir(parents=True, exist_ok=True)

        # Index file will be created dynamically with correct dimensions when needed
        # Don't create it here with hardcoded dimensions
        if not self.defaultMetadataPath.exists():
            with open(self.defaultMetadataPath, 'wb') as f:
                pickle.dump([], f)
            logging.info(f"Created missing vector metadata file at {self.defaultMetadataPath}")

    def create_widgets(self) -> None:
        """Create and layout GUI widgets."""
        self.notebook = ttk.Notebook(self.master)
        self.notebook.pack(expand=True, fill='both')

    # Combined File Processing Tab
        self.fileProcessingTab = FileProcessingTab(self.notebook, self) 
        self.notebook.add(self.fileProcessingTab, text='File Processing')

        # LM Studio Integration Tab
        self.lmStudioTab = LMStudioTab(self.notebook, self)
        self.notebook.add(self.lmStudioTab, text='LM Studio Integration')

    def update_status(self, message: str) -> None:
        """Update the text in the status bar."""
        self.statusBarText.set(message)
        
    def cleanup_resources(self):
        """Clean up resources when app closes"""
        self.update_status("Cleaning up resources...")
        
        try:
            # Release sentence transformer model
            if self.sentenceTransformerModel is not None:
                del self.sentenceTransformerModel
                self.sentenceTransformerModel = None
                logging.info("SentenceTransformer model released from memory")
            
            # Clear all cached models from SentenceTransformerEmbedder
            SentenceTransformerEmbedder.release_models()
            
            # Force garbage collection
            gc.collect()
            
            self.update_status("Resources cleaned up")
        except Exception as e:
            logging.error(f"Error during cleanup: {e}", exc_info=True)
        
    def on_closing(self):
        """Handle window closing event"""
        try:
            self.cleanup_resources()
        finally:
            self.master.destroy()

if __name__ == "__main__":
    root = tk.Tk()
    app = RtfProcessingApp(root)
    root.mainloop()

## config.py ##

# DarthVector/config.py
from pydantic import BaseModel, Field, ConfigDict, model_validator
import os
from pathlib import Path
from dotenv import load_dotenv
from typing import List

# Load environment variables from .env file
load_dotenv()

class Configuration(BaseModel):
    """Configuration settings for the application."""
    model_config = ConfigDict(arbitrary_types_allowed=True)

    defaultOutputDir: Path = Path(os.getenv("DEFAULT_OUTPUT_DIR", "output"))
    defaultIndexName: str = os.getenv("DEFAULT_INDEX_NAME", "vector_index.faiss")
    defaultMetadataName: str = os.getenv("DEFAULT_METADATA_NAME", "metadata.pkl")
    defaultIndexPath: Path = defaultOutputDir / defaultIndexName
    defaultMetadataPath: Path = defaultOutputDir / defaultMetadataName
    defaultModelName: str = os.getenv("MODEL_NAME", "all-mpnet-base-v2")
    defaultMaxWorkers: int = int(os.getenv("MAX_WORKERS", "4"))
    defaultChunkSize: int = int(os.getenv("CHUNK_SIZE", "500"))
    defaultChunkOverlap: int = int(os.getenv("CHUNK_OVERLAP", "50"))
    maxFileSizeMB: int = Field(default=100, description="Maximum file size in MB")
    searchResultCount: int = int(os.getenv("SEARCH_RESULT_COUNT", "5"))
    loggingLevel: str = os.getenv("LOGGING_LEVEL", "INFO").upper()
    loggingFormat: str = os.getenv("LOGGING_FORMAT", "%(asctime)s - %(levelname)s - %(message)s")
    lmStudioApiUrl: str = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
    lmStudioModelName: str = os.getenv("LMSTUDIO_MODEL_NAME", "default")
    defaultInputDir: Path = Path(os.getenv("DEFAULT_INPUT_DIR", ""))
    disableMultiprocessing: bool = Field(default=False, description="Disable multiprocessing", env="DISABLE_MULTIPROCESSING")
    defaultIngestedFilesPath: Path = defaultOutputDir / "ingested_files.json"
    supportedExtensions: List[str] = Field(default=['.rtf', '.pdf', '.docx'], description="Supported file extensions")
    lmStudioMaxTokens: int = Field(default=2000, description="Maximum tokens for LM Studio response", env="LMSTUDIO_MAX_TOKENS")
    rtfEncodings: List[str] = Field(default=['utf-8', 'latin-1', 'cp1252', 'utf-16', 'utf-16-le'], description="Supported RTF encodings", env="RTF_ENCODINGS")

    @model_validator(mode='after')
    def validate_fields(self) -> 'Configuration':
        """Validate configuration fields."""
        # Normalize logging level
        self.loggingLevel = self.loggingLevel.strip().upper()
        valid_logging_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        if self.loggingLevel not in valid_logging_levels:
            raise ValueError(f"Invalid logging level: {self.loggingLevel}. Valid options: {valid_logging_levels}")
        if self.defaultMaxWorkers <= 0:
            raise ValueError("MAX_WORKERS must be positive")
        if self.defaultChunkSize <= 0:
            raise ValueError("CHUNK_SIZE must be positive")
        if self.defaultChunkOverlap < 0 or self.defaultChunkOverlap >= self.defaultChunkSize:
            raise ValueError("CHUNK_OVERLAP must be non-negative and less than CHUNK_SIZE")
        if self.maxFileSizeMB <= 0:
            raise ValueError("maxFileSizeMB must be positive")
        if self.searchResultCount <= 0:
            raise ValueError("SEARCH_RESULT_COUNT must be positive")
        if self.lmStudioMaxTokens <= 0:
            raise ValueError("LMSTUDIO_MAX_TOKENS must be positive")
        if not self.rtfEncodings:
            raise ValueError("RTF_ENCODINGS must not be empty")
        return self

    @classmethod
    def ensureOutputDir(cls) -> None:
        """Ensure the default output directory exists."""
        instance = cls()
        instance.defaultOutputDir.mkdir(exist_ok=True, parents=True)

# Instantiate the configuration
config = Configuration()#### Other Files ####

## requirements.txt ##

# ---------------------------
# File: requirements.txt
# ---------------------------
faiss-gpu
mypy>=1.11.0
numpy>=1.26.4
python-dotenv>=1.0.1
pytest>=8.3.2
sentence-transformers>=3.0.1
setuptools>=70.0.0
striprtf>=0.0.26
tqdm>=4.66.5
wheel>=0.43.0
pydantic>=2.8.2
pyperclip>=1.9.0
pdfplumber>=0.11.4
python-docx>=1.1.2

===== Folder Structure =====

DarthVector/
    updateVectorStoreTab.py
    embeddingGenerator.py
    textExtractor.py
    testPipeline.py
    vectorStore.py
    fileProcessingTab.py
    .env
    lmStudioTab.py
    main.py
    textPreprocessor.py
    requirements.txt
    exceptions.py
    gui.log
    rtfProcessor.py
    .gitignore
    gui.py
    config.py
